{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73ad83e",
   "metadata": {},
   "source": [
    "# Session 2: Data Cleaning and Preprocessing\n",
    "\n",
    "**Data Science with Python - 2025 Edition**\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "By the end of this session, you will be able to:\n",
    "- Remove uncertainty values and handle missing data\n",
    "- Detect and handle outliers effectively\n",
    "- Fix inconsistent data formats and standardize entries\n",
    "- Remove duplicates and correct data entry errors\n",
    "- Handle categorical inconsistencies\n",
    "- Resolve data type mismatches\n",
    "- Standardize units and scales\n",
    "- Filter irrelevant or noisy data\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Why Data Cleaning Matters\n",
    "\n",
    "**\"Garbage in, garbage out\"** - This famous saying perfectly captures why data cleaning is crucial in data science.\n",
    "\n",
    "- Real-world data is **messy** and **incomplete**\n",
    "- Poor data quality leads to **incorrect insights**\n",
    "- Data cleaning can take **60-80%** of a data scientist's time\n",
    "- Clean data = **Better models** and **Reliable results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0118f535",
   "metadata": {},
   "source": [
    "## 🛠️ Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"Libraries imported successfully! We are ready to clean our data!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69684b4",
   "metadata": {},
   "source": [
    "## 📊 Create Sample Messy Dataset\n",
    "\n",
    "Let's create a realistic messy dataset that contains all the common data quality issues we'll address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265b4975",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m data = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m7\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m1\u001b[39m],  \u001b[38;5;66;03m# Duplicate ID at the end\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mJohn\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mjane\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mALICE\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBob\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDiana\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mEve\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFrank\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mJohn\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdepartment\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mIT\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFinance\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mIT\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMarketing\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mHR\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfinance\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mit\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mIT\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m }\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create DataFrame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df = \u001b[43mpd\u001b[49m.DataFrame(data)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreated messy dataset with these problems:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m- Question marks (?) for missing values\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a simple messy dataset\n",
    "data = {\n",
    "    'id': [1, 2, 3, 4, 5, 6, 7, 8, 1],  # Duplicate ID at the end\n",
    "    'name': ['John', 'jane', 'ALICE', 'Bob', '?', 'Diana', 'Eve', 'Frank', 'John'],\n",
    "    'age': [25, 30, '?', 35, -999, 28, 45, 150, 25],\n",
    "    'gender': ['Male', 'female', 'F', 'M', '?', 'Female', 'M', 'F', 'Male'],\n",
    "    'salary': [50000, 60000, 75000, None, 80000, 55000, 9999999, 45000, 50000],\n",
    "    'department': ['IT', 'hr', 'Finance', 'IT', 'Marketing', 'HR', 'finance', 'it', 'IT']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Created messy dataset with these problems:\")\n",
    "print(\"- Question marks (?) for missing values\")\n",
    "print(\"- -999 as placeholder for missing age\") \n",
    "print(\"- Different case letters (john, ALICE)\")\n",
    "print(\"- Very high salary (9999999)\")\n",
    "print(\"- Very high age (150)\")\n",
    "print(\"- Duplicate rows\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba075440",
   "metadata": {},
   "source": [
    "## 🔍 Initial Data Exploration\n",
    "\n",
    "Before cleaning, let's understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384d0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our data\n",
    "print(\"Dataset shape (rows, columns):\", df.shape)\n",
    "print(\"Data types:\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1674bb96",
   "metadata": {},
   "source": [
    "## 🧹 Step 1: Uncertainty Removal\n",
    "\n",
    "First, let's identify and handle uncertainty values like '?', 'unknown', and placeholder values like '-999':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c7c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for uncertainty values like '?' and -999\n",
    "print(\"Names with '?':\", (df['name'] == '?').sum())\n",
    "print(\"Ages with '?':\", (df['age'] == '?').sum()) \n",
    "print(\"Genders with '?':\", (df['gender'] == '?').sum())\n",
    "print(\"Ages with -999:\", (df['age'] == -999).sum())\n",
    "\n",
    "# Show rows with problems\n",
    "df[df['age'] == -999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2894713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of our data to clean\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Replace '?' with missing values (NaN)\n",
    "df_clean = df_clean.replace('?', np.nan)\n",
    "\n",
    "# Replace -999 with missing values\n",
    "df_clean = df_clean.replace(-999, np.nan)\n",
    "\n",
    "print(\"After removing uncertainty values:\")\n",
    "df_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e1208",
   "metadata": {},
   "source": [
    "## 🕳️ Step 2: Treating Missing Values\n",
    "\n",
    "Now let's handle the missing values using appropriate strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values\n",
    "df_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e94bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing names with 'Unknown'\n",
    "df_clean['name'] = df_clean['name'].fillna('Unknown')\n",
    "\n",
    "# Convert age to numbers first\n",
    "df_clean['age'] = pd.to_numeric(df_clean['age'], errors='coerce')\n",
    "\n",
    "# Fill missing ages with the average age\n",
    "average_age = df_clean['age'].mean()\n",
    "df_clean['age'] = df_clean['age'].fillna(average_age)\n",
    "\n",
    "# Fill missing salaries with the average salary\n",
    "average_salary = df_clean['salary'].mean()\n",
    "df_clean['salary'] = df_clean['salary'].fillna(average_salary)\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3426c0",
   "metadata": {},
   "source": [
    "## 📊 Step 3: Handling Outliers\n",
    "\n",
    "Let's identify and handle outliers in numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef206c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for unrealistic ages\n",
    "df_clean['age'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix unrealistic ages (over 100 becomes average age)\n",
    "average_age = df_clean['age'].mean()\n",
    "df_clean.loc[df_clean['age'] > 100, 'age'] = average_age\n",
    "\n",
    "# Fix unrealistic salaries (over 500,000 becomes average salary)\n",
    "average_salary = df_clean['salary'].mean()\n",
    "df_clean.loc[df_clean['salary'] > 500000, 'salary'] = average_salary\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ffd52f",
   "metadata": {},
   "source": [
    "## 📅 Step 4: Fixing Text Cases\n",
    "\n",
    "Let's make sure names and departments have consistent formatting:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35dc612",
   "metadata": {},
   "source": [
    "## 🔄 Step 5: Removing Duplicate Entries\n",
    "\n",
    "Let's identify and handle duplicate records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(\"Total rows:\", len(df_clean))\n",
    "print(\"Duplicate rows:\", df_clean.duplicated().sum())\n",
    "print(\"Duplicate IDs:\", df_clean['id'].duplicated().sum())\n",
    "\n",
    "# Show duplicate rows\n",
    "df_clean[df_clean.duplicated(keep=False)].sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1dae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows (keep the first one)\n",
    "before_count = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "after_count = len(df_clean)\n",
    "\n",
    "print(\"Removed\", before_count - after_count, \"duplicate rows\")\n",
    "\n",
    "# Make names look nice (Title Case)\n",
    "df_clean['name'] = df_clean['name'].str.title()\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f2594",
   "metadata": {},
   "source": [
    "## ✏️ Step 6: Correcting Data Entry Errors\n",
    "\n",
    "Let's identify and fix common data entry errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15111a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for reasonable age range (18 to 80)\n",
    "weird_ages = df_clean[(df_clean['age'] < 18) | (df_clean['age'] > 80)]\n",
    "print(\"Ages outside normal range (18-80):\", len(weird_ages))\n",
    "\n",
    "# Check for reasonable salary range  \n",
    "weird_salaries = df_clean[(df_clean['salary'] < 20000) | (df_clean['salary'] > 200000)]\n",
    "print(\"Salaries outside normal range (20,000 - 200,000):\", len(weird_salaries))\n",
    "\n",
    "weird_salaries if len(weird_salaries) > 0 else \"All salaries look good!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b436ffb",
   "metadata": {},
   "source": [
    "## 🏷️ Step 7: Dealing with Categorical Inconsistencies\n",
    "\n",
    "Let's standardize categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at gender values\n",
    "df_clean['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f418132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make gender values consistent\n",
    "df_clean['gender'] = df_clean['gender'].replace('F', 'Female')\n",
    "df_clean['gender'] = df_clean['gender'].replace('M', 'Male')\n",
    "df_clean['gender'] = df_clean['gender'].replace('female', 'Female')\n",
    "\n",
    "# Make department names consistent (proper case)\n",
    "df_clean['department'] = df_clean['department'].replace('hr', 'HR')\n",
    "df_clean['department'] = df_clean['department'].replace('it', 'IT')\n",
    "df_clean['department'] = df_clean['department'].replace('finance', 'Finance')\n",
    "\n",
    "df_clean['department'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56f01e",
   "metadata": {},
   "source": [
    "## 🔢 Step 8: Resolving Data Type Mismatches\n",
    "\n",
    "Let's ensure all columns have the correct data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f60b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert age to whole numbers\n",
    "df_clean['age'] = df_clean['age'].round().astype(int)\n",
    "\n",
    "# Convert salary to whole numbers  \n",
    "df_clean['salary'] = df_clean['salary'].round().astype(int)\n",
    "\n",
    "df_clean.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b66aa",
   "metadata": {},
   "source": [
    "## 🔍 Step 9: Final Check\n",
    "\n",
    "Let's make sure our data looks good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check of our clean data\n",
    "print(\"Shape:\", df_clean.shape)\n",
    "print(\"Missing values:\", df_clean.isnull().sum().sum())\n",
    "print(\"Age range:\", df_clean['age'].min(), \"to\", df_clean['age'].max())\n",
    "print(\"Salary range: $\", df_clean['salary'].min(), \"to $\", df_clean['salary'].max())\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90468a0",
   "metadata": {},
   "source": [
    "## 📊 Before vs After Comparison\n",
    "\n",
    "Let's see how much we improved our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d803b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs cleaned data\n",
    "print(\"BEFORE CLEANING:\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Missing values:\", df.isnull().sum().sum())\n",
    "print(\"Data types:\", list(df.dtypes))\n",
    "print()\n",
    "\n",
    "print(\"AFTER CLEANING:\")\n",
    "print(\"Shape:\", df_clean.shape)  \n",
    "print(\"Missing values:\", df_clean.isnull().sum().sum())\n",
    "print(\"Data types:\", list(df_clean.dtypes))\n",
    "print()\n",
    "\n",
    "print(\"IMPROVEMENTS MADE:\")\n",
    "print(\"✓ Removed uncertainty values (?, -999)\")\n",
    "print(\"✓ Filled missing values with averages\")\n",
    "print(\"✓ Fixed unrealistic ages and salaries\") \n",
    "print(\"✓ Removed duplicate rows\")\n",
    "print(\"✓ Standardized text formats\")\n",
    "print(\"✓ Made gender and department names consistent\")\n",
    "print(\"✓ Converted to proper data types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb316ef",
   "metadata": {},
   "source": [
    "## 💾 Save Cleaned Dataset\n",
    "\n",
    "Let's save our cleaned dataset for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our clean data\n",
    "df_clean.to_csv('clean_employee_data.csv', index=False)\n",
    "\n",
    "\"✅ Saved clean data to 'clean_employee_data.csv' - Ready for analysis!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609ef945",
   "metadata": {},
   "source": [
    "## 🎓 What We Learned\n",
    "\n",
    "### Data Cleaning Steps We Did:\n",
    "1. ✅ **Found Problem Values**: Looked for '?', -999, and missing data\n",
    "2. ✅ **Fixed Missing Values**: Replaced with averages or 'Unknown'\n",
    "3. ✅ **Fixed Unrealistic Values**: Changed very high ages and salaries\n",
    "4. ✅ **Removed Duplicates**: Kept only unique rows\n",
    "5. ✅ **Fixed Text Formatting**: Made names and departments consistent\n",
    "6. ✅ **Standardized Categories**: Made M/F into Male/Female\n",
    "7. ✅ **Fixed Data Types**: Made sure numbers are numbers\n",
    "\n",
    "### Why Data Cleaning is Important:\n",
    "- **Real data is messy** - it has errors and missing pieces\n",
    "- **Clean data = better results** - your analysis will be more accurate\n",
    "- **It takes time** - but it's worth it!\n",
    "\n",
    "### Remember:\n",
    "- Always look at your data first\n",
    "- Make a copy before cleaning\n",
    "- Check your work at each step\n",
    "- Document what you did\n",
    "\n",
    "### What's Next:\n",
    "- Now we can analyze our clean data\n",
    "- Make charts and graphs\n",
    "- Find patterns and insights\n",
    "- Build models\n",
    "\n",
    "**Great job! You've learned the basics of data cleaning! \udf89**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
