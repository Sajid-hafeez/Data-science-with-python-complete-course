{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0dca972",
   "metadata": {},
   "source": [
    "# Session 3: Categorical Variables and Encoding\n",
    "\n",
    "**Data Science with Python - 2025 Edition**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this session, you will:\n",
    "- Understand different types of categorical variables\n",
    "- Learn when to use Label Encoding vs One-Hot Encoding\n",
    "- Apply encoding techniques to real datasets\n",
    "- Prepare categorical data for machine learning models\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Topics Covered\n",
    "1. **Types of Categorical Variables**\n",
    "   - Nominal vs Ordinal\n",
    "   - Examples and identification\n",
    "\n",
    "2. **Label Encoding**\n",
    "   - When to use it\n",
    "   - Implementation with examples\n",
    "\n",
    "3. **One-Hot Encoding**\n",
    "   - When to use it\n",
    "   - Implementation with examples\n",
    "\n",
    "**Let's start exploring categorical data! üìä**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247a124",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed49d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\"Libraries loaded! Ready to work with categorical variables! üìä\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b870df43",
   "metadata": {},
   "source": [
    "## üìä What are Categorical Variables?\n",
    "\n",
    "**Categorical variables** represent data that can be divided into groups or categories. Unlike numerical variables, they don't have mathematical meaning.\n",
    "\n",
    "### Examples:\n",
    "- **Colors**: Red, Blue, Green\n",
    "- **Gender**: Male, Female\n",
    "- **Education**: High School, Bachelor's, Master's\n",
    "- **Ratings**: Poor, Good, Excellent\n",
    "\n",
    "---\n",
    "\n",
    "## üè∑Ô∏è Types of Categorical Variables\n",
    "\n",
    "There are **two main types** of categorical variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33304e32",
   "metadata": {},
   "source": [
    "### 1. üî¢ Nominal Categorical Variables\n",
    "\n",
    "**Definition**: Categories with **NO natural order** or ranking.\n",
    "\n",
    "**Characteristics**:\n",
    "- Categories are just different labels\n",
    "- No category is \"higher\" or \"better\" than another\n",
    "- Order doesn't matter\n",
    "\n",
    "**Examples**:\n",
    "- **Colors**: Red, Blue, Green, Yellow\n",
    "- **Car Brands**: Toyota, BMW, Honda, Ford\n",
    "- **Countries**: USA, India, Japan, Germany\n",
    "- **Blood Types**: A, B, AB, O\n",
    "\n",
    "**Key Point**: Red is not \"greater than\" Blue - they're just different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Nominal Categorical Data\n",
    "colors = ['Red', 'Blue', 'Green', 'Red', 'Yellow', 'Blue', 'Green']\n",
    "car_brands = ['Toyota', 'BMW', 'Honda', 'Toyota', 'Ford', 'BMW']\n",
    "\n",
    "nominal_data = pd.DataFrame({\n",
    "    'Color': colors,\n",
    "    'Car_Brand': car_brands[:7]  # Match the length\n",
    "})\n",
    "\n",
    "nominal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22604d12",
   "metadata": {},
   "source": [
    "### 2. üìà Ordinal Categorical Variables\n",
    "\n",
    "**Definition**: Categories with a **clear natural order** or ranking.\n",
    "\n",
    "**Characteristics**:\n",
    "- Categories can be ranked from low to high\n",
    "- There's a meaningful sequence\n",
    "- Order matters!\n",
    "\n",
    "**Examples**:\n",
    "- **Education Level**: High School < Bachelor's < Master's < PhD\n",
    "- **Ratings**: Poor < Fair < Good < Excellent\n",
    "- **Income Level**: Low < Medium < High\n",
    "- **T-shirt Sizes**: Small < Medium < Large < XL\n",
    "\n",
    "**Key Point**: \"Excellent\" is definitely better than \"Poor\" - there's a clear order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Ordinal Categorical Data\n",
    "education = ['High School', 'Bachelor', 'Master', 'PhD', 'Bachelor', 'High School']\n",
    "ratings = ['Poor', 'Good', 'Excellent', 'Fair', 'Good', 'Excellent']\n",
    "sizes = ['Small', 'Medium', 'Large', 'XL', 'Medium', 'Small']\n",
    "\n",
    "ordinal_data = pd.DataFrame({\n",
    "    'Education': education,\n",
    "    'Rating': ratings,\n",
    "    'Size': sizes\n",
    "})\n",
    "\n",
    "ordinal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed145e5",
   "metadata": {},
   "source": [
    "## üîç Quick Quiz: Identify the Type!\n",
    "\n",
    "Look at these variables and think about whether they are **Nominal** or **Ordinal**:\n",
    "\n",
    "1. **Movie Genres**: Action, Comedy, Drama, Horror\n",
    "2. **Grade Levels**: A, B, C, D, F\n",
    "3. **Cities**: New York, London, Tokyo, Paris\n",
    "4. **Temperature**: Cold, Warm, Hot\n",
    "5. **Marital Status**: Single, Married, Divorced\n",
    "\n",
    "**Answers**:\n",
    "1. **Nominal** - No natural order between genres\n",
    "2. **Ordinal** - A > B > C > D > F (clear ranking)\n",
    "3. **Nominal** - Cities have no inherent order\n",
    "4. **Ordinal** - Cold < Warm < Hot (temperature order)\n",
    "5. **Nominal** - No natural ranking between marital statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93a69d",
   "metadata": {},
   "source": [
    "## ü§ñ Why Do We Need Encoding?\n",
    "\n",
    "**Problem**: Machine learning algorithms work with numbers, not text!\n",
    "\n",
    "**Solution**: Convert categorical variables to numerical format through **encoding**.\n",
    "\n",
    "### Two Main Encoding Techniques:\n",
    "1. **Label Encoding** - For ordinal variables\n",
    "2. **One-Hot Encoding** - For nominal variables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61edb4c",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Label Encoding\n",
    "\n",
    "**What is Label Encoding?**\n",
    "- Converts categories to numbers (0, 1, 2, 3, ...)\n",
    "- Each unique category gets a unique number\n",
    "- **Best for ordinal variables** where order matters\n",
    "\n",
    "**When to Use Label Encoding:**\n",
    "- ‚úÖ Ordinal categorical variables (education, ratings, sizes)\n",
    "- ‚úÖ When the order of categories is meaningful\n",
    "- ‚úÖ Target variables in classification problems\n",
    "\n",
    "**When NOT to Use:**\n",
    "- ‚ùå Nominal variables (the algorithm might think there's an order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c54ee",
   "metadata": {},
   "source": [
    "### üìù Example 1: Education Level (Ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47642f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample education data\n",
    "education_data = pd.DataFrame({\n",
    "    'Education': ['High School', 'Bachelor', 'Master', 'PhD', 'Bachelor', 'High School', 'Master']\n",
    "})\n",
    "\n",
    "education_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Manual Label Encoding (when you want control over the order)\n",
    "education_mapping = {\n",
    "    'High School': 0,\n",
    "    'Bachelor': 1,\n",
    "    'Master': 2,\n",
    "    'PhD': 3\n",
    "}\n",
    "\n",
    "education_data['Education_Encoded'] = education_data['Education'].map(education_mapping)\n",
    "education_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f0017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using sklearn LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "education_data['Education_LabelEncoder'] = label_encoder.fit_transform(education_data['Education'])\n",
    "\n",
    "education_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dea839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the mapping created by LabelEncoder\n",
    "mapping_df = pd.DataFrame({\n",
    "    'Original': label_encoder.classes_,\n",
    "    'Encoded': range(len(label_encoder.classes_))\n",
    "})\n",
    "\n",
    "mapping_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f8fb9",
   "metadata": {},
   "source": [
    "### üìù Example 2: Customer Ratings (Ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer ratings data\n",
    "ratings_data = pd.DataFrame({\n",
    "    'Customer_ID': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'Rating': ['Poor', 'Good', 'Excellent', 'Fair', 'Good', 'Poor', 'Excellent', 'Fair']\n",
    "})\n",
    "\n",
    "ratings_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual encoding with meaningful order\n",
    "rating_mapping = {\n",
    "    'Poor': 1,\n",
    "    'Fair': 2, \n",
    "    'Good': 3,\n",
    "    'Excellent': 4\n",
    "}\n",
    "\n",
    "ratings_data['Rating_Encoded'] = ratings_data['Rating'].map(rating_mapping)\n",
    "ratings_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395d634",
   "metadata": {},
   "source": [
    "### üéØ Key Benefits of Label Encoding:\n",
    "- **Preserves order** for ordinal variables\n",
    "- **Memory efficient** (only one column)\n",
    "- **Simple to implement**\n",
    "- **Reversible** (can decode back to original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding back to original values\n",
    "ratings_data['Rating_Decoded'] = ratings_data['Rating_Encoded'].map({v: k for k, v in rating_mapping.items()})\n",
    "ratings_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad9ff8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé≠ One-Hot Encoding\n",
    "\n",
    "**What is One-Hot Encoding?**\n",
    "- Creates **separate binary columns** for each category\n",
    "- Each row has 1 in one column and 0 in all others\n",
    "- **Best for nominal variables** where no order exists\n",
    "\n",
    "**When to Use One-Hot Encoding:**\n",
    "- ‚úÖ Nominal categorical variables (colors, countries, brands)\n",
    "- ‚úÖ When categories have no meaningful order\n",
    "- ‚úÖ Most machine learning algorithms prefer this for nominal data\n",
    "\n",
    "**When NOT to Use:**\n",
    "- ‚ùå High cardinality variables (too many categories = too many columns)\n",
    "- ‚ùå Ordinal variables (loses the order information)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4404a7",
   "metadata": {},
   "source": [
    "### üìù Example 1: Car Colors (Nominal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create car color data\n",
    "car_data = pd.DataFrame({\n",
    "    'Car_ID': [1, 2, 3, 4, 5, 6],\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Red', 'Yellow', 'Blue']\n",
    "})\n",
    "\n",
    "car_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using pandas get_dummies (most common)\n",
    "car_encoded = pd.get_dummies(car_data, columns=['Color'], prefix='Color')\n",
    "car_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00641d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using sklearn OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "color_encoded = onehot_encoder.fit_transform(car_data[['Color']])\n",
    "\n",
    "# Create DataFrame with proper column names\n",
    "color_columns = [f'Color_{cat}' for cat in onehot_encoder.categories_[0]]\n",
    "color_encoded_df = pd.DataFrame(color_encoded, columns=color_columns)\n",
    "\n",
    "# Combine with original data\n",
    "car_sklearn = pd.concat([car_data[['Car_ID']], color_encoded_df], axis=1)\n",
    "car_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd5968",
   "metadata": {},
   "source": [
    "### üìù Example 2: Customer Countries (Nominal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer country data\n",
    "customer_data = pd.DataFrame({\n",
    "    'Customer_ID': [101, 102, 103, 104, 105, 106, 107],\n",
    "    'Country': ['USA', 'India', 'Germany', 'USA', 'Japan', 'India', 'Germany'],\n",
    "    'Age': [25, 30, 35, 28, 45, 32, 38]\n",
    "})\n",
    "\n",
    "customer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode only the Country column\n",
    "customer_encoded = pd.get_dummies(customer_data, columns=['Country'], prefix='Country')\n",
    "customer_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639d86e",
   "metadata": {},
   "source": [
    "### üéØ Key Benefits of One-Hot Encoding:\n",
    "- **No false ordering** imposed on nominal variables\n",
    "- **Clear representation** - easy to interpret\n",
    "- **Works well** with most ML algorithms\n",
    "- **Prevents bias** from arbitrary numerical assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6083a",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Label Encoding vs One-Hot Encoding\n",
    "\n",
    "### üìä Comparison Table\n",
    "\n",
    "| Aspect | Label Encoding | One-Hot Encoding |\n",
    "|--------|----------------|------------------|\n",
    "| **Best For** | Ordinal Variables | Nominal Variables |\n",
    "| **Output** | Single column with numbers | Multiple binary columns |\n",
    "| **Memory Usage** | Low | Higher |\n",
    "| **Preserves Order** | Yes | No |\n",
    "| **Risk of False Order** | No (when used correctly) | No |\n",
    "| **Algorithm Preference** | Tree-based models | Linear models |\n",
    "\n",
    "### üéØ Decision Guide:\n",
    "\n",
    "**Use Label Encoding when:**\n",
    "- Variable is ordinal (has natural order)\n",
    "- You want to preserve the ranking\n",
    "- Memory efficiency is important\n",
    "\n",
    "**Use One-Hot Encoding when:**\n",
    "- Variable is nominal (no natural order)\n",
    "- You want to avoid false ordering\n",
    "- Working with linear algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1309133d",
   "metadata": {},
   "source": [
    "## üöó Practical Example: Complete Car Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive car dataset\n",
    "car_dataset = pd.DataFrame({\n",
    "    'Brand': ['Toyota', 'BMW', 'Honda', 'Ford', 'Toyota', 'BMW', 'Honda'],\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Red', 'Black', 'White', 'Blue'],\n",
    "    'Size': ['Small', 'Large', 'Medium', 'Large', 'Medium', 'Large', 'Small'],\n",
    "    'Condition': ['Poor', 'Excellent', 'Good', 'Fair', 'Good', 'Excellent', 'Fair'],\n",
    "    'Price': [15000, 45000, 25000, 35000, 22000, 50000, 18000]\n",
    "})\n",
    "\n",
    "car_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cee0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify variable types\n",
    "variable_types = {\n",
    "    'Brand': 'Nominal',\n",
    "    'Color': 'Nominal', \n",
    "    'Size': 'Ordinal',\n",
    "    'Condition': 'Ordinal',\n",
    "    'Price': 'Numerical'\n",
    "}\n",
    "\n",
    "for var, var_type in variable_types.items():\n",
    "    print(f\"{var}: {var_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply appropriate encoding\n",
    "car_processed = car_dataset.copy()\n",
    "\n",
    "# 1. One-hot encode nominal variables\n",
    "car_processed = pd.get_dummies(car_processed, columns=['Brand', 'Color'], prefix=['Brand', 'Color'])\n",
    "\n",
    "# 2. Label encode ordinal variables\n",
    "size_mapping = {'Small': 1, 'Medium': 2, 'Large': 3}\n",
    "condition_mapping = {'Poor': 1, 'Fair': 2, 'Good': 3, 'Excellent': 4}\n",
    "\n",
    "car_processed['Size_Encoded'] = car_processed['Size'].map(size_mapping)\n",
    "car_processed['Condition_Encoded'] = car_processed['Condition'].map(condition_mapping)\n",
    "\n",
    "# Drop original categorical columns\n",
    "car_processed = car_processed.drop(['Size', 'Condition'], axis=1)\n",
    "\n",
    "car_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0410eb",
   "metadata": {},
   "source": [
    "## üé™ Hands-On Exercise: Employee Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create employee dataset for practice\n",
    "employee_data = pd.DataFrame({\n",
    "    'Employee_ID': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'Department': ['HR', 'IT', 'Finance', 'IT', 'Marketing', 'HR', 'Finance', 'Marketing'],\n",
    "    'Education': ['Bachelor', 'Master', 'PhD', 'Bachelor', 'Master', 'High School', 'PhD', 'Bachelor'],\n",
    "    'Performance': ['Good', 'Excellent', 'Poor', 'Good', 'Fair', 'Excellent', 'Good', 'Fair'],\n",
    "    'Salary': [50000, 80000, 35000, 55000, 60000, 45000, 90000, 52000]\n",
    "})\n",
    "\n",
    "employee_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041dfdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify which encoding to use for each variable\n",
    "print(\"Variable Type Analysis:\")\n",
    "print(\"Department: Nominal (no ranking between departments)\")\n",
    "print(\"Education: Ordinal (High School < Bachelor < Master < PhD)\")\n",
    "print(\"Performance: Ordinal (Poor < Fair < Good < Excellent)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c65826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply encodings\n",
    "employee_processed = employee_data.copy()\n",
    "\n",
    "# One-hot encode Department (nominal)\n",
    "employee_processed = pd.get_dummies(employee_processed, columns=['Department'], prefix='Dept')\n",
    "\n",
    "# Label encode Education (ordinal)\n",
    "education_order = {'High School': 1, 'Bachelor': 2, 'Master': 3, 'PhD': 4}\n",
    "employee_processed['Education_Encoded'] = employee_processed['Education'].map(education_order)\n",
    "\n",
    "# Label encode Performance (ordinal)\n",
    "performance_order = {'Poor': 1, 'Fair': 2, 'Good': 3, 'Excellent': 4}\n",
    "employee_processed['Performance_Encoded'] = employee_processed['Performance'].map(performance_order)\n",
    "\n",
    "# Drop original categorical columns\n",
    "employee_processed = employee_processed.drop(['Education', 'Performance'], axis=1)\n",
    "\n",
    "employee_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b52cb",
   "metadata": {},
   "source": [
    "## üìä Visualizing the Impact of Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba1500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs encoded data\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original Performance distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "employee_data['Performance'].value_counts().plot(kind='bar', color='lightblue')\n",
    "plt.title('Original Performance Categories')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Encoded Performance distribution  \n",
    "plt.subplot(1, 3, 2)\n",
    "employee_processed['Performance_Encoded'].value_counts().sort_index().plot(kind='bar', color='lightgreen')\n",
    "plt.title('Label Encoded Performance')\n",
    "plt.xlabel('Encoded Values')\n",
    "\n",
    "# Department one-hot encoding visualization\n",
    "plt.subplot(1, 3, 3)\n",
    "dept_cols = [col for col in employee_processed.columns if col.startswith('Dept_')]\n",
    "employee_processed[dept_cols].sum().plot(kind='bar', color='orange')\n",
    "plt.title('One-Hot Encoded Departments')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b460e9",
   "metadata": {},
   "source": [
    "## üéØ Best Practices and Tips\n",
    "\n",
    "### ‚úÖ Do's:\n",
    "1. **Always identify** whether your categorical variable is nominal or ordinal first\n",
    "2. **Use Label Encoding** for ordinal variables to preserve order\n",
    "3. **Use One-Hot Encoding** for nominal variables to avoid false ordering\n",
    "4. **Check for high cardinality** before one-hot encoding (too many categories)\n",
    "5. **Save your mappings** for future use (encoding/decoding)\n",
    "\n",
    "### ‚ùå Don'ts:\n",
    "1. **Don't use Label Encoding** on nominal variables with many categories\n",
    "2. **Don't use One-Hot Encoding** on high cardinality variables (>10-15 categories)\n",
    "3. **Don't forget** to apply the same encoding to test/new data\n",
    "4. **Don't lose** the original categorical data (keep backups)\n",
    "\n",
    "### üö® Common Mistakes:\n",
    "- Using Label Encoding on nominal variables (creates false order)\n",
    "- One-hot encoding ordinal variables (loses order information)\n",
    "- Not handling unseen categories in new data\n",
    "- Creating too many dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c318cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîß Feature Scaling and Preprocessing\n",
    "\n",
    "Now that we've mastered categorical encoding, let's explore **numerical feature preprocessing**!\n",
    "\n",
    "**Why Scale Features?**\n",
    "- Different features have different scales (age: 20-80, salary: 20,000-100,000)\n",
    "- Machine learning algorithms can be sensitive to feature scales\n",
    "- Scaling ensures all features contribute equally to the model\n",
    "\n",
    "## üìä Types of Scaling We'll Cover:\n",
    "1. **Binarization** - Convert numerical to binary (0/1)\n",
    "2. **Standardization (Z-score)** - Mean=0, Std=1\n",
    "3. **Normalization (Min-Max)** - Scale to [0,1] range  \n",
    "4. **Robust Scaling** - Uses median and IQR (less sensitive to outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset for scaling examples\n",
    "np.random.seed(42)\n",
    "\n",
    "scaling_data = pd.DataFrame({\n",
    "    'Age': [25, 35, 28, 45, 32, 55, 29, 38, 42, 31],\n",
    "    'Salary': [35000, 85000, 45000, 120000, 55000, 150000, 40000, 95000, 110000, 60000], \n",
    "    'Experience': [1, 8, 3, 15, 5, 20, 2, 10, 12, 4],\n",
    "    'Score': [65, 88, 72, 95, 78, 92, 68, 85, 90, 75]\n",
    "})\n",
    "\n",
    "scaling_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ee77b",
   "metadata": {},
   "source": [
    "## üîÑ Binarization of Numerical Features\n",
    "\n",
    "**What is Binarization?**\n",
    "- Converts numerical values to binary (0 or 1) based on a threshold\n",
    "- Values above threshold = 1, values below = 0\n",
    "- Useful for creating yes/no features from continuous data\n",
    "\n",
    "**When to Use Binarization:**\n",
    "- ‚úÖ When you only care about \"above/below\" a certain value\n",
    "- ‚úÖ Creating flags or indicators (high/low income, pass/fail scores)\n",
    "- ‚úÖ Simplifying complex numerical relationships\n",
    "- ‚úÖ When the exact value matters less than the category\n",
    "\n",
    "**Examples:**\n",
    "- Age ‚Üí Senior (1 if age ‚â• 60, else 0)\n",
    "- Score ‚Üí Pass (1 if score ‚â• 70, else 0)\n",
    "- Income ‚Üí High Income (1 if income ‚â• 80000, else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Manual Binarization\n",
    "binarized_data = scaling_data.copy()\n",
    "\n",
    "# Create binary features based on thresholds\n",
    "binarized_data['Is_Senior'] = (binarized_data['Age'] >= 40).astype(int)\n",
    "binarized_data['High_Salary'] = (binarized_data['Salary'] >= 80000).astype(int)\n",
    "binarized_data['Experienced'] = (binarized_data['Experience'] >= 10).astype(int)\n",
    "binarized_data['High_Score'] = (binarized_data['Score'] >= 80).astype(int)\n",
    "\n",
    "binarized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72615e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using sklearn Binarizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Binarize Age with threshold 40\n",
    "age_binarizer = Binarizer(threshold=40)\n",
    "scaling_data['Age_Binary'] = age_binarizer.fit_transform(scaling_data[['Age']])\n",
    "\n",
    "# Binarize Score with threshold 80\n",
    "score_binarizer = Binarizer(threshold=80)\n",
    "scaling_data['Score_Binary'] = score_binarizer.fit_transform(scaling_data[['Score']])\n",
    "\n",
    "scaling_data[['Age', 'Age_Binary', 'Score', 'Score_Binary']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4a3af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìè Standardization using Z-score Scaling\n",
    "\n",
    "**What is Standardization (Z-score Scaling)?**\n",
    "- Transforms features to have **mean = 0** and **standard deviation = 1**\n",
    "- Formula: **z = (x - Œº) / œÉ**\n",
    "  - z = standardized value\n",
    "  - x = original value\n",
    "  - Œº = mean of the feature\n",
    "  - œÉ = standard deviation of the feature\n",
    "\n",
    "**When to Use Standardization:**\n",
    "- ‚úÖ Features have different scales (age vs salary)\n",
    "- ‚úÖ Algorithm assumes normally distributed data\n",
    "- ‚úÖ Using algorithms sensitive to scale (SVM, Neural Networks, PCA)\n",
    "- ‚úÖ When you want to preserve the shape of the distribution\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Preserves outliers** (doesn't compress them)\n",
    "- **Centers data around 0**\n",
    "- **Works well** with algorithms that assume normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Manual Z-score Standardization\n",
    "standardized_data = scaling_data.copy()\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "salary_mean = standardized_data['Salary'].mean()\n",
    "salary_std = standardized_data['Salary'].std()\n",
    "\n",
    "# Apply Z-score formula manually\n",
    "standardized_data['Salary_Standardized_Manual'] = (standardized_data['Salary'] - salary_mean) / salary_std\n",
    "\n",
    "# Show the calculation\n",
    "print(f\"Salary Mean: {salary_mean:,.2f}\")\n",
    "print(f\"Salary Std: {salary_std:,.2f}\")\n",
    "print(\"\\nOriginal vs Standardized:\")\n",
    "standardized_data[['Salary', 'Salary_Standardized_Manual']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using sklearn StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create and fit the scaler\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = ['Age', 'Salary', 'Experience', 'Score']\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(standardized_data[numerical_columns])\n",
    "\n",
    "# Create DataFrame with scaled data\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=[f'{col}_Standardized' for col in numerical_columns])\n",
    "\n",
    "# Combine with original data\n",
    "result = pd.concat([standardized_data[numerical_columns], scaled_df], axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify standardization properties (mean ‚âà 0, std ‚âà 1)\n",
    "print(\"Standardization Verification:\")\n",
    "print(\"Mean of standardized features (should be ‚âà 0):\")\n",
    "print(scaled_df.mean().round(10))\n",
    "print(\"\\nStandard deviation of standardized features (should be ‚âà 1):\")  \n",
    "print(scaled_df.std().round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4e38f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìê Normalization using Min-Max Scaling\n",
    "\n",
    "**What is Min-Max Normalization?**\n",
    "- Scales features to a **fixed range [0, 1]**\n",
    "- Formula: **x_norm = (x - min) / (max - min)**\n",
    "  - x_norm = normalized value\n",
    "  - x = original value  \n",
    "  - min = minimum value in the feature\n",
    "  - max = maximum value in the feature\n",
    "\n",
    "**When to Use Min-Max Scaling:**\n",
    "- ‚úÖ You want features in a specific range [0, 1]\n",
    "- ‚úÖ Data is uniformly distributed\n",
    "- ‚úÖ Using algorithms that work better with bounded features (Neural Networks)\n",
    "- ‚úÖ When you know the approximate range of future data\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Preserves relationships** between data points\n",
    "- **Bounded output** (always between 0 and 1)\n",
    "- **No assumptions** about data distribution\n",
    "- **Sensitive to outliers** (can compress most data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Manual Min-Max Normalization\n",
    "normalized_data = scaling_data.copy()\n",
    "\n",
    "# Apply Min-Max formula manually for Salary\n",
    "salary_min = normalized_data['Salary'].min()\n",
    "salary_max = normalized_data['Salary'].max()\n",
    "\n",
    "normalized_data['Salary_Normalized_Manual'] = (normalized_data['Salary'] - salary_min) / (salary_max - salary_min)\n",
    "\n",
    "# Show the calculation\n",
    "print(f\"Salary Min: {salary_min:,}\")\n",
    "print(f\"Salary Max: {salary_max:,}\")\n",
    "print(f\"Range: {salary_max - salary_min:,}\")\n",
    "print(\"\\nOriginal vs Normalized:\")\n",
    "normalized_data[['Salary', 'Salary_Normalized_Manual']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c667bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using sklearn MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create and fit the scaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "numerical_columns = ['Age', 'Salary', 'Experience', 'Score']\n",
    "\n",
    "# Fit and transform the data\n",
    "normalized_data_sklearn = minmax_scaler.fit_transform(normalized_data[numerical_columns])\n",
    "\n",
    "# Create DataFrame with normalized data\n",
    "normalized_df = pd.DataFrame(normalized_data_sklearn, columns=[f'{col}_Normalized' for col in numerical_columns])\n",
    "\n",
    "# Combine with original data\n",
    "result_normalized = pd.concat([normalized_data[numerical_columns], normalized_df], axis=1)\n",
    "result_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Min-Max normalization properties (range [0, 1])\n",
    "print(\"Min-Max Normalization Verification:\")\n",
    "print(\"Minimum values (should be 0):\")\n",
    "print(normalized_df.min())\n",
    "print(\"\\nMaximum values (should be 1):\")\n",
    "print(normalized_df.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510afea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ°Ô∏è Robust Scaling using Median and IQR\n",
    "\n",
    "**What is Robust Scaling?**\n",
    "- Uses **median** and **Interquartile Range (IQR)** instead of mean and std\n",
    "- Formula: **x_robust = (x - median) / IQR**\n",
    "  - x_robust = robust scaled value\n",
    "  - x = original value\n",
    "  - median = middle value (50th percentile)\n",
    "  - IQR = Q3 - Q1 (75th percentile - 25th percentile)\n",
    "\n",
    "**When to Use Robust Scaling:**\n",
    "- ‚úÖ Data contains **outliers**\n",
    "- ‚úÖ Data is **not normally distributed**\n",
    "- ‚úÖ You want scaling **resistant to extreme values**\n",
    "- ‚úÖ When median is more representative than mean\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Less sensitive to outliers** than StandardScaler\n",
    "- **Uses robust statistics** (median, IQR)\n",
    "- **Preserves outliers** but doesn't let them dominate\n",
    "- **Works well** with skewed distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with outliers to demonstrate robust scaling\n",
    "data_with_outliers = pd.DataFrame({\n",
    "    'Normal_Income': [45000, 50000, 48000, 52000, 47000, 49000, 51000, 46000],\n",
    "    'Income_With_Outlier': [45000, 50000, 48000, 52000, 47000, 49000, 51000, 500000],  # Last value is outlier\n",
    "    'Normal_Age': [25, 30, 28, 32, 27, 29, 31, 26],\n",
    "    'Age_With_Outlier': [25, 30, 28, 32, 27, 29, 31, 95]  # Last value is outlier\n",
    "})\n",
    "\n",
    "data_with_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c80a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Manual Robust Scaling\n",
    "robust_data = data_with_outliers.copy()\n",
    "\n",
    "# Calculate robust statistics for Income_With_Outlier\n",
    "income_median = robust_data['Income_With_Outlier'].median()\n",
    "income_q1 = robust_data['Income_With_Outlier'].quantile(0.25)\n",
    "income_q3 = robust_data['Income_With_Outlier'].quantile(0.75)\n",
    "income_iqr = income_q3 - income_q1\n",
    "\n",
    "# Apply robust scaling formula\n",
    "robust_data['Income_Robust_Manual'] = (robust_data['Income_With_Outlier'] - income_median) / income_iqr\n",
    "\n",
    "# Show the statistics\n",
    "print(f\"Income Median: {income_median:,.2f}\")\n",
    "print(f\"Income Q1: {income_q1:,.2f}\")\n",
    "print(f\"Income Q3: {income_q3:,.2f}\")\n",
    "print(f\"Income IQR: {income_iqr:,.2f}\")\n",
    "print(\"\\nOriginal vs Robust Scaled:\")\n",
    "robust_data[['Income_With_Outlier', 'Income_Robust_Manual']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e1854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using sklearn RobustScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Create and fit the robust scaler\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Apply to all numerical columns\n",
    "robust_scaled_data = robust_scaler.fit_transform(data_with_outliers)\n",
    "\n",
    "# Create DataFrame with robust scaled data\n",
    "robust_df = pd.DataFrame(robust_scaled_data, columns=[f'{col}_Robust' for col in data_with_outliers.columns])\n",
    "\n",
    "# Combine with original data\n",
    "result_robust = pd.concat([data_with_outliers, robust_df], axis=1)\n",
    "result_robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd18632",
   "metadata": {},
   "source": [
    "### üìä Comparing Scaling Methods with Outliers\n",
    "\n",
    "Let's see how different scaling methods handle the outlier (500,000 income):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6707b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all scaling methods on data with outliers\n",
    "comparison_data = data_with_outliers[['Income_With_Outlier']].copy()\n",
    "\n",
    "# Apply all scaling methods\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "comparison_data['Standardized'] = standard_scaler.fit_transform(comparison_data[['Income_With_Outlier']])\n",
    "comparison_data['Normalized'] = minmax_scaler.fit_transform(comparison_data[['Income_With_Outlier']])\n",
    "comparison_data['Robust_Scaled'] = robust_scaler.fit_transform(comparison_data[['Income_With_Outlier']])\n",
    "\n",
    "comparison_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41625d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Complete Comparison: All Scaling Methods\n",
    "\n",
    "### üìã Scaling Methods Comparison Table\n",
    "\n",
    "| Method | Output Range | Best For | Sensitive to Outliers | Preserves Distribution |\n",
    "|--------|--------------|----------|---------------------|----------------------|\n",
    "| **Binarization** | {0, 1} | Binary flags, threshold-based features | No | No |\n",
    "| **Standardization** | Unbounded (mean=0, std=1) | Normal distributions, SVM, Neural Networks | Yes | Yes |\n",
    "| **Min-Max** | [0, 1] | Bounded features, uniform distributions | Very sensitive | Yes |\n",
    "| **Robust** | Unbounded (median-centered) | Data with outliers, skewed distributions | No | Yes |\n",
    "\n",
    "### üéØ Decision Guide:\n",
    "\n",
    "**Use Binarization when:**\n",
    "- You only care about above/below threshold\n",
    "- Creating binary flags or indicators\n",
    "- Simplifying numerical relationships\n",
    "\n",
    "**Use Standardization when:**\n",
    "- Data is approximately normal\n",
    "- Using algorithms that assume normal distribution\n",
    "- Features have very different scales\n",
    "\n",
    "**Use Min-Max when:**\n",
    "- You need bounded output [0,1]\n",
    "- Data is uniformly distributed\n",
    "- No significant outliers present\n",
    "\n",
    "**Use Robust Scaling when:**\n",
    "- Data contains outliers\n",
    "- Distribution is skewed\n",
    "- You want outlier-resistant scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5010372",
   "metadata": {},
   "source": [
    "## üé™ Hands-On Exercise: Complete Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68abb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive dataset for practice\n",
    "complete_dataset = pd.DataFrame({\n",
    "    'Customer_ID': range(1, 11),\n",
    "    'Age': [25, 35, 28, 45, 32, 55, 29, 38, 42, 31],\n",
    "    'Income': [35000, 85000, 45000, 120000, 55000, 250000, 40000, 95000, 110000, 60000],  # Contains outlier\n",
    "    'Education': ['Bachelor', 'Master', 'High School', 'PhD', 'Bachelor', 'Master', 'High School', 'PhD', 'Master', 'Bachelor'],\n",
    "    'City': ['New York', 'London', 'Tokyo', 'Paris', 'New York', 'London', 'Berlin', 'Tokyo', 'Paris', 'Berlin'],\n",
    "    'Satisfaction': ['Good', 'Excellent', 'Poor', 'Good', 'Fair', 'Excellent', 'Poor', 'Good', 'Fair', 'Excellent'],\n",
    "    'Years_Experience': [2, 12, 5, 20, 8, 25, 3, 15, 18, 6],\n",
    "    'Premium_Customer': [0, 1, 0, 1, 0, 1, 0, 1, 1, 0]  # Already binary\n",
    "})\n",
    "\n",
    "complete_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bbef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply complete preprocessing pipeline\n",
    "processed_dataset = complete_dataset.copy()\n",
    "\n",
    "# Step 1: Identify variable types\n",
    "print(\"Variable Types:\")\n",
    "print(\"Age: Numerical (for scaling)\")\n",
    "print(\"Income: Numerical with outlier (for robust scaling)\")\n",
    "print(\"Education: Ordinal (High School < Bachelor < Master < PhD)\")\n",
    "print(\"City: Nominal (no natural order)\")\n",
    "print(\"Satisfaction: Ordinal (Poor < Fair < Good < Excellent)\")\n",
    "print(\"Years_Experience: Numerical (for binarization and scaling)\")\n",
    "print(\"Premium_Customer: Already binary\")\n",
    "\n",
    "# Step 2: One-hot encode nominal variables\n",
    "processed_dataset = pd.get_dummies(processed_dataset, columns=['City'], prefix='City')\n",
    "\n",
    "# Step 3: Label encode ordinal variables\n",
    "education_mapping = {'High School': 1, 'Bachelor': 2, 'Master': 3, 'PhD': 4}\n",
    "satisfaction_mapping = {'Poor': 1, 'Fair': 2, 'Good': 3, 'Excellent': 4}\n",
    "\n",
    "processed_dataset['Education_Encoded'] = processed_dataset['Education'].map(education_mapping)\n",
    "processed_dataset['Satisfaction_Encoded'] = processed_dataset['Satisfaction'].map(satisfaction_mapping)\n",
    "\n",
    "# Step 4: Binarize Years_Experience (experienced = 10+ years)\n",
    "processed_dataset['Is_Experienced'] = (processed_dataset['Years_Experience'] >= 10).astype(int)\n",
    "\n",
    "# Step 5: Scale numerical features\n",
    "# Use robust scaling for Income (has outlier), standard scaling for Age\n",
    "robust_scaler = RobustScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "processed_dataset['Income_Robust_Scaled'] = robust_scaler.fit_transform(processed_dataset[['Income']])\n",
    "processed_dataset['Age_Standardized'] = standard_scaler.fit_transform(processed_dataset[['Age']])\n",
    "\n",
    "# Drop original categorical columns\n",
    "processed_dataset = processed_dataset.drop(['Education', 'Satisfaction'], axis=1)\n",
    "\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the preprocessing results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Original Income distribution\n",
    "axes[0, 0].hist(complete_dataset['Income'], bins=5, alpha=0.7, color='red')\n",
    "axes[0, 0].set_title('Original Income (with outlier)')\n",
    "axes[0, 0].set_xlabel('Income')\n",
    "\n",
    "# Robust scaled Income\n",
    "axes[0, 1].hist(processed_dataset['Income_Robust_Scaled'], bins=5, alpha=0.7, color='green')\n",
    "axes[0, 1].set_title('Robust Scaled Income')\n",
    "axes[0, 1].set_xlabel('Scaled Income')\n",
    "\n",
    "# Original Age distribution\n",
    "axes[0, 2].hist(complete_dataset['Age'], bins=5, alpha=0.7, color='blue')\n",
    "axes[0, 2].set_title('Original Age')\n",
    "axes[0, 2].set_xlabel('Age')\n",
    "\n",
    "# Standardized Age\n",
    "axes[1, 0].hist(processed_dataset['Age_Standardized'], bins=5, alpha=0.7, color='orange')\n",
    "axes[1, 0].set_title('Standardized Age')\n",
    "axes[1, 0].set_xlabel('Standardized Age')\n",
    "\n",
    "# Education encoding\n",
    "education_counts = processed_dataset['Education_Encoded'].value_counts().sort_index()\n",
    "axes[1, 1].bar(education_counts.index, education_counts.values, color='purple')\n",
    "axes[1, 1].set_title('Education Level Encoding')\n",
    "axes[1, 1].set_xlabel('Encoded Education')\n",
    "axes[1, 1].set_xticks([1, 2, 3, 4])\n",
    "axes[1, 1].set_xticklabels(['HS', 'Bach', 'Mast', 'PhD'])\n",
    "\n",
    "# City one-hot encoding\n",
    "city_cols = [col for col in processed_dataset.columns if col.startswith('City_')]\n",
    "city_sums = processed_dataset[city_cols].sum()\n",
    "axes[1, 2].bar(range(len(city_sums)), city_sums.values, color='teal')\n",
    "axes[1, 2].set_title('One-Hot Encoded Cities')\n",
    "axes[1, 2].set_xlabel('Cities')\n",
    "axes[1, 2].set_xticks(range(len(city_sums)))\n",
    "axes[1, 2].set_xticklabels([col.replace('City_', '') for col in city_cols], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f4203b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üö® Outlier Detection and Removal\n",
    "\n",
    "**What are Outliers?**\n",
    "- Data points that are significantly different from other observations\n",
    "- Can be caused by measurement errors, data entry mistakes, or genuine extreme values\n",
    "- Can negatively impact machine learning model performance\n",
    "\n",
    "**Why Remove Outliers?**\n",
    "- Improve model accuracy and stability\n",
    "- Prevent skewed statistics (mean, standard deviation)\n",
    "- Reduce noise in the dataset\n",
    "- Better visualization and interpretation\n",
    "\n",
    "## üîç Methods for Outlier Detection and Removal:\n",
    "1. **IQR Method** - Using Interquartile Range\n",
    "2. **Z-score Method** - Using standard deviations from mean\n",
    "3. **Percentile Thresholds** - Using custom percentile cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b10efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with clear outliers for demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate normal data\n",
    "normal_data = np.random.normal(50, 10, 100)\n",
    "\n",
    "# Add some obvious outliers\n",
    "outliers = [150, 200, -20, -50, 180]\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data_with_outliers = np.concatenate([normal_data, outliers])\n",
    "\n",
    "outlier_dataset = pd.DataFrame({\n",
    "    'ID': range(1, len(data_with_outliers) + 1),\n",
    "    'Value': data_with_outliers,\n",
    "    'Category': ['Normal'] * len(normal_data) + ['Outlier'] * len(outliers)\n",
    "})\n",
    "\n",
    "# Show some statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Count: {len(outlier_dataset)}\")\n",
    "print(f\"Mean: {outlier_dataset['Value'].mean():.2f}\")\n",
    "print(f\"Median: {outlier_dataset['Value'].median():.2f}\")\n",
    "print(f\"Std: {outlier_dataset['Value'].std():.2f}\")\n",
    "print(f\"Min: {outlier_dataset['Value'].min():.2f}\")\n",
    "print(f\"Max: {outlier_dataset['Value'].max():.2f}\")\n",
    "\n",
    "outlier_dataset.tail(10)  # Show last 10 rows to see outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e688e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data with outliers\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Box plot\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.boxplot(outlier_dataset['Value'])\n",
    "plt.title('Box Plot - Outliers Visible')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(outlier_dataset['Value'], bins=20, alpha=0.7, color='skyblue')\n",
    "plt.title('Histogram - Distribution with Outliers')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Scatter plot with color coding\n",
    "plt.subplot(1, 3, 3)\n",
    "colors = ['red' if cat == 'Outlier' else 'blue' for cat in outlier_dataset['Category']]\n",
    "plt.scatter(outlier_dataset['ID'], outlier_dataset['Value'], c=colors, alpha=0.6)\n",
    "plt.title('Scatter Plot - Red = Outliers')\n",
    "plt.xlabel('ID')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81c1347",
   "metadata": {},
   "source": [
    "## üìä Method 1: IQR (Interquartile Range) Method\n",
    "\n",
    "**What is IQR Method?**\n",
    "- Uses the concept of quartiles (Q1, Q3) to identify outliers\n",
    "- **IQR = Q3 - Q1** (75th percentile - 25th percentile)\n",
    "- **Lower Bound = Q1 - 1.5 √ó IQR**\n",
    "- **Upper Bound = Q3 + 1.5 √ó IQR**\n",
    "- Any value outside these bounds is considered an outlier\n",
    "\n",
    "**When to Use IQR Method:**\n",
    "- ‚úÖ Data doesn't need to be normally distributed\n",
    "- ‚úÖ Robust to extreme values (uses quartiles)\n",
    "- ‚úÖ Most commonly used method\n",
    "- ‚úÖ Works well with skewed distributions\n",
    "\n",
    "**Advantages:**\n",
    "- **Non-parametric** (no distribution assumptions)\n",
    "- **Robust** to extreme outliers\n",
    "- **Easy to understand** and implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944db25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR Method Implementation\n",
    "def remove_outliers_iqr(data, column_name):\n",
    "    \"\"\"\n",
    "    Remove outliers using IQR method\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Calculate Q1, Q3, and IQR\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Calculate bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]\n",
    "    \n",
    "    # Remove outliers\n",
    "    cleaned_data = df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)]\n",
    "    \n",
    "    print(f\"IQR Method Results:\")\n",
    "    print(f\"Q1: {Q1:.2f}\")\n",
    "    print(f\"Q3: {Q3:.2f}\")\n",
    "    print(f\"IQR: {IQR:.2f}\")\n",
    "    print(f\"Lower Bound: {lower_bound:.2f}\")\n",
    "    print(f\"Upper Bound: {upper_bound:.2f}\")\n",
    "    print(f\"Original Data Points: {len(df)}\")\n",
    "    print(f\"Outliers Detected: {len(outliers)}\")\n",
    "    print(f\"Data Points After Cleaning: {len(cleaned_data)}\")\n",
    "    \n",
    "    return cleaned_data, outliers\n",
    "\n",
    "# Apply IQR method\n",
    "cleaned_iqr, outliers_iqr = remove_outliers_iqr(outlier_dataset, 'Value')\n",
    "cleaned_iqr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9454e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìè Method 2: Z-score Method\n",
    "\n",
    "**What is Z-score Method?**\n",
    "- Uses standard deviations from the mean to identify outliers\n",
    "- **Z-score = (x - Œº) / œÉ**\n",
    "- Typically, **|Z-score| > 3** indicates an outlier\n",
    "- Sometimes **|Z-score| > 2** is used for more aggressive outlier removal\n",
    "\n",
    "**When to Use Z-score Method:**\n",
    "- ‚úÖ Data is approximately normally distributed\n",
    "- ‚úÖ You want to use statistical significance\n",
    "- ‚úÖ Working with standardized data\n",
    "- ‚úÖ Need precise control over outlier sensitivity\n",
    "\n",
    "**Advantages:**\n",
    "- **Statistical basis** (based on normal distribution)\n",
    "- **Adjustable threshold** (2, 2.5, 3 standard deviations)\n",
    "- **Works well** with normally distributed data\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Assumes normal distribution**\n",
    "- **Sensitive to extreme outliers** (they affect mean and std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c19a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score Method Implementation\n",
    "def remove_outliers_zscore(data, column_name, threshold=3):\n",
    "    \"\"\"\n",
    "    Remove outliers using Z-score method\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    mean = df[column_name].mean()\n",
    "    std = df[column_name].std()\n",
    "    \n",
    "    # Calculate Z-scores\n",
    "    df['Z_Score'] = np.abs((df[column_name] - mean) / std)\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = df[df['Z_Score'] > threshold]\n",
    "    \n",
    "    # Remove outliers\n",
    "    cleaned_data = df[df['Z_Score'] <= threshold].drop('Z_Score', axis=1)\n",
    "    \n",
    "    print(f\"Z-score Method Results (threshold = {threshold}):\")\n",
    "    print(f\"Mean: {mean:.2f}\")\n",
    "    print(f\"Standard Deviation: {std:.2f}\")\n",
    "    print(f\"Z-score Threshold: {threshold}\")\n",
    "    print(f\"Original Data Points: {len(df)}\")\n",
    "    print(f\"Outliers Detected: {len(outliers)}\")\n",
    "    print(f\"Data Points After Cleaning: {len(cleaned_data)}\")\n",
    "    \n",
    "    return cleaned_data, outliers.drop('Z_Score', axis=1)\n",
    "\n",
    "# Apply Z-score method with different thresholds\n",
    "print(\"=== Z-score Method with Threshold = 3 ===\")\n",
    "cleaned_z3, outliers_z3 = remove_outliers_zscore(outlier_dataset, 'Value', threshold=3)\n",
    "\n",
    "print(\"\\n=== Z-score Method with Threshold = 2 ===\")\n",
    "cleaned_z2, outliers_z2 = remove_outliers_zscore(outlier_dataset, 'Value', threshold=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36324486",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìê Method 3: Percentile Thresholds\n",
    "\n",
    "**What is Percentile Threshold Method?**\n",
    "- Uses custom percentile cutoffs to define outlier boundaries\n",
    "- Common approaches:\n",
    "  - **Remove top/bottom 5%** (5th and 95th percentiles)\n",
    "  - **Remove top/bottom 1%** (1st and 99th percentiles)\n",
    "  - **Custom thresholds** based on domain knowledge\n",
    "\n",
    "**When to Use Percentile Method:**\n",
    "- ‚úÖ You want to remove a specific percentage of extreme values\n",
    "- ‚úÖ Domain knowledge suggests certain cutoffs\n",
    "- ‚úÖ Simple and interpretable approach\n",
    "- ‚úÖ Works with any distribution\n",
    "\n",
    "**Advantages:**\n",
    "- **Flexible** (you control the percentage)\n",
    "- **Simple to understand**\n",
    "- **No distribution assumptions**\n",
    "- **Predictable results** (exact percentage removed)\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Arbitrary cutoffs** (might remove valid data)\n",
    "- **Fixed percentage** (might not reflect actual outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c3ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile Threshold Method Implementation\n",
    "def remove_outliers_percentile(data, column_name, lower_percentile=5, upper_percentile=95):\n",
    "    \"\"\"\n",
    "    Remove outliers using percentile thresholds\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Calculate percentile thresholds\n",
    "    lower_threshold = df[column_name].quantile(lower_percentile / 100)\n",
    "    upper_threshold = df[column_name].quantile(upper_percentile / 100)\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = df[(df[column_name] < lower_threshold) | (df[column_name] > upper_threshold)]\n",
    "    \n",
    "    # Remove outliers\n",
    "    cleaned_data = df[(df[column_name] >= lower_threshold) & (df[column_name] <= upper_threshold)]\n",
    "    \n",
    "    print(f\"Percentile Method Results ({lower_percentile}th - {upper_percentile}th percentile):\")\n",
    "    print(f\"Lower Threshold ({lower_percentile}th percentile): {lower_threshold:.2f}\")\n",
    "    print(f\"Upper Threshold ({upper_percentile}th percentile): {upper_threshold:.2f}\")\n",
    "    print(f\"Original Data Points: {len(df)}\")\n",
    "    print(f\"Outliers Detected: {len(outliers)}\")\n",
    "    print(f\"Data Points After Cleaning: {len(cleaned_data)}\")\n",
    "    print(f\"Percentage Removed: {(len(outliers)/len(df)*100):.1f}%\")\n",
    "    \n",
    "    return cleaned_data, outliers\n",
    "\n",
    "# Apply percentile method with different thresholds\n",
    "print(\"=== Percentile Method: Remove top/bottom 5% ===\")\n",
    "cleaned_p5, outliers_p5 = remove_outliers_percentile(outlier_dataset, 'Value', 5, 95)\n",
    "\n",
    "print(\"\\n=== Percentile Method: Remove top/bottom 1% ===\")\n",
    "cleaned_p1, outliers_p1 = remove_outliers_percentile(outlier_dataset, 'Value', 1, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all outlier removal methods\n",
    "comparison_results = pd.DataFrame({\n",
    "    'Method': ['Original', 'IQR', 'Z-score (3)', 'Z-score (2)', 'Percentile (5-95)', 'Percentile (1-99)'],\n",
    "    'Data_Points': [\n",
    "        len(outlier_dataset),\n",
    "        len(cleaned_iqr),\n",
    "        len(cleaned_z3),\n",
    "        len(cleaned_z2),\n",
    "        len(cleaned_p5),\n",
    "        len(cleaned_p1)\n",
    "    ],\n",
    "    'Outliers_Removed': [\n",
    "        0,\n",
    "        len(outliers_iqr),\n",
    "        len(outliers_z3),\n",
    "        len(outliers_z2),\n",
    "        len(outliers_p5),\n",
    "        len(outliers_p1)\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison_results['Percentage_Removed'] = (comparison_results['Outliers_Removed'] / len(outlier_dataset) * 100).round(1)\n",
    "\n",
    "comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32331545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison of all methods\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "datasets = [\n",
    "    (outlier_dataset, 'Original Data'),\n",
    "    (cleaned_iqr, 'IQR Method'),\n",
    "    (cleaned_z3, 'Z-score (threshold=3)'),\n",
    "    (cleaned_z2, 'Z-score (threshold=2)'),\n",
    "    (cleaned_p5, 'Percentile (5-95%)'),\n",
    "    (cleaned_p1, 'Percentile (1-99%)')\n",
    "]\n",
    "\n",
    "for i, (data, title) in enumerate(datasets):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    # Box plot for each method\n",
    "    axes[row, col].boxplot(data['Value'])\n",
    "    axes[row, col].set_title(f'{title}\\n({len(data)} points)')\n",
    "    axes[row, col].set_ylabel('Value')\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = data['Value'].mean()\n",
    "    median_val = data['Value'].median()\n",
    "    std_val = data['Value'].std()\n",
    "    axes[row, col].text(0.02, 0.98, f'Mean: {mean_val:.1f}\\nMedian: {median_val:.1f}\\nStd: {std_val:.1f}',\n",
    "                       transform=axes[row, col].transAxes, fontsize=9,\n",
    "                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b8473",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîß Building Preprocessing Pipelines using Scikit-learn\n",
    "\n",
    "**What are Preprocessing Pipelines?**\n",
    "- A sequence of data transformation steps applied in order\n",
    "- Ensures consistent preprocessing across training and test data\n",
    "- Prevents data leakage and makes preprocessing reproducible\n",
    "- Combines multiple preprocessing steps into a single object\n",
    "\n",
    "**Benefits of Pipelines:**\n",
    "- **Reproducibility** - Same transformations applied consistently\n",
    "- **No Data Leakage** - Fit only on training data, transform on test data\n",
    "- **Clean Code** - All preprocessing in one place\n",
    "- **Easy Deployment** - Save and load entire pipeline\n",
    "- **Parameter Tuning** - Can tune preprocessing parameters with models\n",
    "\n",
    "## üèóÔ∏è Components We'll Use:\n",
    "1. **ColumnTransformer** - Apply different transformations to different columns\n",
    "2. **Pipeline** - Chain multiple steps together\n",
    "3. **Custom Transformers** - Create our own transformation steps\n",
    "4. **StandardScaler, OneHotEncoder, etc.** - Built-in transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad3ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive dataset for pipeline demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "pipeline_data = pd.DataFrame({\n",
    "    'Age': [25, 35, 28, 45, 32, 55, 29, 38, 42, 31, 150, 22],  # Contains outlier (150)\n",
    "    'Income': [35000, 85000, 45000, 120000, 55000, 250000, 40000, 95000, 110000, 60000, 50000, 30000],  # Contains outlier\n",
    "    'Education': ['Bachelor', 'Master', 'High School', 'PhD', 'Bachelor', 'Master', 'High School', 'PhD', 'Master', 'Bachelor', 'Bachelor', 'High School'],\n",
    "    'City': ['New York', 'London', 'Tokyo', 'Paris', 'New York', 'London', 'Berlin', 'Tokyo', 'Paris', 'Berlin', 'London', 'Tokyo'],\n",
    "    'Satisfaction': ['Good', 'Excellent', 'Poor', 'Good', 'Fair', 'Excellent', 'Poor', 'Good', 'Fair', 'Excellent', 'Good', 'Fair'],\n",
    "    'Years_Experience': [2, 12, 5, 20, 8, 25, 3, 15, 18, 6, 4, 1],\n",
    "    'Premium_Customer': [0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0],  # Target variable\n",
    "})\n",
    "\n",
    "print(\"Pipeline Dataset:\")\n",
    "print(f\"Shape: {pipeline_data.shape}\")\n",
    "pipeline_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd9f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Create custom transformer for outlier removal\n",
    "class IQROutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer to remove outliers using IQR method\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        self.bounds_ = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if self.columns is None:\n",
    "            self.columns = X.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        for col in self.columns:\n",
    "            Q1 = X[col].quantile(0.25)\n",
    "            Q3 = X[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            self.bounds_[col] = (lower_bound, upper_bound)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        for col in self.columns:\n",
    "            if col in self.bounds_:\n",
    "                lower_bound, upper_bound = self.bounds_[col]\n",
    "                # Remove outliers (keep only rows within bounds)\n",
    "                mask = (X_transformed[col] >= lower_bound) & (X_transformed[col] <= upper_bound)\n",
    "                X_transformed = X_transformed[mask]\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "# Test the custom transformer\n",
    "outlier_remover = IQROutlierRemover(columns=['Age', 'Income'])\n",
    "outlier_remover.fit(pipeline_data)\n",
    "\n",
    "print(\"Outlier bounds calculated:\")\n",
    "for col, (lower, upper) in outlier_remover.bounds_.items():\n",
    "    print(f\"{col}: [{lower:.2f}, {upper:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comprehensive preprocessing pipeline\n",
    "\n",
    "# Step 1: Separate features and target\n",
    "X = pipeline_data.drop('Premium_Customer', axis=1)\n",
    "y = pipeline_data['Premium_Customer']\n",
    "\n",
    "# Step 2: Define column types\n",
    "numerical_columns = ['Age', 'Income', 'Years_Experience']\n",
    "ordinal_columns = ['Education', 'Satisfaction']  \n",
    "nominal_columns = ['City']\n",
    "\n",
    "# Step 3: Define ordinal mappings\n",
    "education_categories = ['High School', 'Bachelor', 'Master', 'PhD']\n",
    "satisfaction_categories = ['Poor', 'Fair', 'Good', 'Excellent']\n",
    "\n",
    "# Step 4: Create preprocessing pipeline\n",
    "preprocessing_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Numerical features: outlier removal + robust scaling\n",
    "        ('numerical', Pipeline([\n",
    "            ('outlier_removal', IQROutlierRemover(columns=['Age', 'Income'])),\n",
    "            ('scaler', RobustScaler())\n",
    "        ]), numerical_columns),\n",
    "        \n",
    "        # Ordinal features: ordinal encoding + standard scaling\n",
    "        ('ordinal', Pipeline([\n",
    "            ('ordinal_encoder', OrdinalEncoder(categories=[education_categories, satisfaction_categories])),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), ordinal_columns),\n",
    "        \n",
    "        # Nominal features: one-hot encoding\n",
    "        ('nominal', OneHotEncoder(drop='first', sparse_output=False), nominal_columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other columns as-is\n",
    ")\n",
    "\n",
    "print(\"Preprocessing Pipeline Created!\")\n",
    "print(\"\\nPipeline Steps:\")\n",
    "print(\"1. Numerical columns: IQR outlier removal + Robust scaling\")\n",
    "print(\"2. Ordinal columns: Ordinal encoding + Standard scaling\") \n",
    "print(\"3. Nominal columns: One-hot encoding\")\n",
    "print(\"\\nColumn assignments:\")\n",
    "print(f\"Numerical: {numerical_columns}\")\n",
    "print(f\"Ordinal: {ordinal_columns}\")\n",
    "print(f\"Nominal: {nominal_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507862a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing pipeline\n",
    "print(\"Original data shape:\", X.shape)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_preprocessed = preprocessing_pipeline.fit_transform(X)\n",
    "print(\"Preprocessed data shape:\", X_preprocessed.shape)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = []\n",
    "\n",
    "# Numerical features (after outlier removal, some rows might be dropped)\n",
    "feature_names.extend([f'{col}_scaled' for col in numerical_columns])\n",
    "\n",
    "# Ordinal features  \n",
    "feature_names.extend([f'{col}_encoded' for col in ordinal_columns])\n",
    "\n",
    "# Nominal features (one-hot encoded, drop first)\n",
    "onehot_encoder = preprocessing_pipeline.named_transformers_['nominal']\n",
    "city_features = onehot_encoder.get_feature_names_out(nominal_columns)\n",
    "feature_names.extend(city_features)\n",
    "\n",
    "# Create DataFrame with preprocessed data\n",
    "X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=feature_names)\n",
    "\n",
    "print(\"\\nPreprocessed Features:\")\n",
    "X_preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28910f",
   "metadata": {},
   "source": [
    "### üéØ Pipeline Benefits Demonstrated\n",
    "\n",
    "**What our pipeline accomplished:**\n",
    "1. **Consistent Processing** - Same transformations applied to all data\n",
    "2. **No Data Leakage** - Fit parameters only on training data\n",
    "3. **Outlier Handling** - Automatically removed outliers using IQR method  \n",
    "4. **Appropriate Encoding** - Different methods for different variable types\n",
    "5. **Feature Scaling** - Normalized features for better model performance\n",
    "6. **Reproducibility** - Can apply same transformations to new data\n",
    "\n",
    "**Key Advantages:**\n",
    "- **Fit once, transform many** - Pipeline remembers all parameters\n",
    "- **Clean separation** - Training vs test data handled properly\n",
    "- **Easy deployment** - Single object contains entire preprocessing\n",
    "- **Parameter tuning** - Can optimize preprocessing with model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate pipeline with new data (simulating test data)\n",
    "new_data = pd.DataFrame({\n",
    "    'Age': [30, 50, 200],  # Third value is an outlier\n",
    "    'Income': [60000, 100000, 1000000],  # Third value is an outlier\n",
    "    'Education': ['Master', 'PhD', 'Bachelor'],\n",
    "    'City': ['London', 'Berlin', 'New York'],\n",
    "    'Satisfaction': ['Good', 'Excellent', 'Fair'],\n",
    "    'Years_Experience': [7, 18, 3]\n",
    "})\n",
    "\n",
    "print(\"New data to transform:\")\n",
    "print(new_data)\n",
    "\n",
    "# Transform new data using fitted pipeline (no fitting again!)\n",
    "new_data_preprocessed = preprocessing_pipeline.transform(new_data)\n",
    "\n",
    "print(f\"\\nOriginal new data shape: {new_data.shape}\")\n",
    "print(f\"Preprocessed new data shape: {new_data_preprocessed.shape}\")\n",
    "print(\"\\nNote: Outlier row was removed automatically!\")\n",
    "\n",
    "# Show preprocessed new data\n",
    "new_data_preprocessed_df = pd.DataFrame(new_data_preprocessed, columns=feature_names)\n",
    "new_data_preprocessed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd241f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Complete Method Comparison Guide\n",
    "\n",
    "### üö® Outlier Removal Methods Comparison\n",
    "\n",
    "| Method | Best For | Pros | Cons | When to Use |\n",
    "|--------|----------|------|------|-------------|\n",
    "| **IQR** | Any distribution | Robust, non-parametric | May remove valid extreme values | General purpose, skewed data |\n",
    "| **Z-score** | Normal distribution | Statistical basis, adjustable | Assumes normality, sensitive to outliers | Normally distributed data |\n",
    "| **Percentile** | Custom control | Simple, predictable | Arbitrary cutoffs | When domain knowledge suggests cutoffs |\n",
    "\n",
    "### üîß Preprocessing Pipeline Benefits\n",
    "\n",
    "| Aspect | Without Pipeline | With Pipeline |\n",
    "|--------|------------------|---------------|\n",
    "| **Consistency** | Manual, error-prone | Automatic, consistent |\n",
    "| **Data Leakage** | Risk of leakage | Prevented by design |\n",
    "| **Reproducibility** | Difficult to reproduce | Easy to reproduce |\n",
    "| **Deployment** | Complex setup | Single object |\n",
    "| **Maintenance** | Scattered code | Centralized preprocessing |\n",
    "\n",
    "### üéØ Decision Framework:\n",
    "\n",
    "**Choose IQR Method when:**\n",
    "- Data is skewed or non-normal\n",
    "- You want robust outlier detection\n",
    "- Working with diverse datasets\n",
    "\n",
    "**Choose Z-score Method when:**\n",
    "- Data is approximately normal\n",
    "- You need statistical justification\n",
    "- Want adjustable sensitivity\n",
    "\n",
    "**Choose Percentile Method when:**\n",
    "- You know the desired percentage to remove\n",
    "- Domain expertise suggests specific cutoffs\n",
    "- Simple interpretability is important\n",
    "\n",
    "**Use Preprocessing Pipelines when:**\n",
    "- Building production ML systems\n",
    "- Need consistent preprocessing\n",
    "- Working with train/test splits\n",
    "- Deploying models to production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98053cd",
   "metadata": {},
   "source": [
    "## üéì Summary\n",
    "\n",
    "### üìã Key Takeaways:\n",
    "\n",
    "1. **Categorical Variables** come in two types:\n",
    "   - **Nominal**: No natural order (colors, brands, countries)\n",
    "   - **Ordinal**: Clear natural order (education, ratings, sizes)\n",
    "\n",
    "2. **Label Encoding**:\n",
    "   - Converts categories to numbers (0, 1, 2, ...)\n",
    "   - Perfect for ordinal variables\n",
    "   - Preserves order and saves memory\n",
    "\n",
    "3. **One-Hot Encoding**:\n",
    "   - Creates binary columns for each category\n",
    "   - Perfect for nominal variables\n",
    "   - Prevents false ordering\n",
    "\n",
    "4. **Binarization**:\n",
    "   - Converts numerical to binary (0/1) based on threshold\n",
    "   - Perfect for creating flags and indicators\n",
    "   - Simplifies complex relationships\n",
    "\n",
    "5. **Standardization (Z-score)**:\n",
    "   - Centers data around mean=0, std=1\n",
    "   - Perfect for normally distributed data\n",
    "   - Works well with SVM, Neural Networks\n",
    "\n",
    "6. **Min-Max Normalization**:\n",
    "   - Scales features to [0,1] range\n",
    "   - Perfect for bounded features\n",
    "   - Sensitive to outliers\n",
    "\n",
    "7. **Robust Scaling**:\n",
    "   - Uses median and IQR instead of mean and std\n",
    "   - Perfect for data with outliers\n",
    "   - Less sensitive to extreme values\n",
    "\n",
    "8. **Outlier Removal Methods**:\n",
    "   - **IQR Method**: Robust, works with any distribution\n",
    "   - **Z-score Method**: Statistical, assumes normal distribution\n",
    "   - **Percentile Method**: Simple, predictable percentage removal\n",
    "\n",
    "9. **Preprocessing Pipelines**:\n",
    "   - Combine multiple preprocessing steps\n",
    "   - Ensure consistency and prevent data leakage\n",
    "   - Enable easy deployment and reproducibility\n",
    "\n",
    "### üéØ Decision Rules:\n",
    "- **Nominal variables** ‚Üí One-Hot Encoding\n",
    "- **Ordinal variables** ‚Üí Label Encoding\n",
    "- **Threshold-based features** ‚Üí Binarization\n",
    "- **Normal distribution** ‚Üí Standardization\n",
    "- **Need bounded output** ‚Üí Min-Max Scaling\n",
    "- **Data with outliers** ‚Üí Robust Scaling\n",
    "- **Outlier removal** ‚Üí IQR (general), Z-score (normal data), Percentile (custom)\n",
    "- **Production systems** ‚Üí Always use Preprocessing Pipelines\n",
    "\n",
    "### üöÄ What's Next:\n",
    "- Feature selection techniques\n",
    "- Advanced encoding methods (Target encoding, Binary encoding)\n",
    "- Cross-validation with preprocessing pipelines\n",
    "- Building complete ML workflows\n",
    "- Model deployment with preprocessing\n",
    "\n",
    "**Congratulations! You now master comprehensive data preprocessing techniques! üéâ**\n",
    "\n",
    "---\n",
    "\n",
    "*Remember: Proper preprocessing is the foundation of successful machine learning. Choose techniques based on your data characteristics and business requirements!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
