{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c14dc81",
   "metadata": {},
   "source": [
    "# Session 5: Linear Regression\n",
    "\n",
    "## Session Introduction & Objectives\n",
    "\n",
    "Welcome to Session 5! Today, we'll dive into one of the most fundamental and widely used algorithms in machine learning: **Linear Regression**. We'll build upon our data analysis and visualization skills to create predictive models.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this session, you will be able to:\n",
    "- Understand the theory behind Linear Regression.\n",
    "- Implement a Linear Regression model using scikit-learn.\n",
    "- Prepare data for a regression model (train-test split, scaling).\n",
    "- Evaluate the performance of a Linear Regression model.\n",
    "- Interpret the results of the model.\n",
    "\n",
    "### What We'll Cover\n",
    "1. **What is Linear Regression?**\n",
    "2. **Loading a Real-World Dataset**\n",
    "3. **Exploratory Data Analysis (EDA)**\n",
    "4. **Data Preprocessing**\n",
    "5. **Training the Linear Regression Model**\n",
    "6. **Model Evaluation**\n",
    "7. **Visualizing Results**\n",
    "8. **Hands-on Practice**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474c367",
   "metadata": {},
   "source": [
    "## 1. What is Linear Regression?\n",
    "\n",
    "Linear Regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. The goal is to find the best-fitting straight line (or hyperplane) that describes the data.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Dependent Variable (Target, y):** The variable we are trying to predict.\n",
    "- **Independent Variable(s) (Features, X):** The variables we use to make the prediction.\n",
    "- **Simple Linear Regression:** One independent variable. The formula is: `y = β₀ + β₁x + ε`\n",
    "- **Multiple Linear Regression:** Two or more independent variables. The formula is: `y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε`\n",
    "\n",
    "Where:\n",
    "- `β₀` is the intercept (the value of y when all x's are 0).\n",
    "- `β₁...βₙ` are the coefficients (the change in y for a one-unit change in x).\n",
    "- `ε` is the error term.\n",
    "\n",
    "### Assumptions of Linear Regression\n",
    "For a linear regression model to be accurate and reliable, several key assumptions about the data must hold true.\n",
    "\n",
    "1.  **Linearity:** The relationship between the independent variables (X) and the dependent variable (y) must be linear. You can check this by creating scatter plots of each feature against the target. If the pattern is not linear, you might need to transform the data (e.g., log transformation) or use a different, non-linear model.\n",
    "\n",
    "2.  **Independence:** The observations in the dataset must be independent of each other. This means that the residual (error) for one observation is not correlated with the residual of another. This assumption is often violated in time-series data, where one observation depends on the previous one.\n",
    "\n",
    "3.  **Homoscedasticity (Constant Variance):** The variance of the residuals should be constant across all levels of the independent variables. In other words, the spread of the errors should be consistent. If the variance increases as the predicted value increases (a funnel shape in the residual plot), this is called heteroscedasticity and can make the model's predictions less reliable.\n",
    "\n",
    "4.  **Normality of Residuals:** The residuals of the model should be normally distributed. This doesn't mean the features or target must be normally distributed, but their errors should be. You can check this by plotting a histogram of the residuals. If this assumption is violated, the p-values and confidence intervals for the coefficients can be unreliable.\n",
    "\n",
    "5.  **No Multicollinearity:** The independent variables should not be highly correlated with each other. If there is high correlation (multicollinearity), it becomes difficult for the model to determine the individual effect of each feature on the target variable. You can check for this using a correlation matrix or by calculating the Variance Inflation Factor (VIF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Set up matplotlib for inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Set default figure size\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9689d728",
   "metadata": {},
   "source": [
    "## 2. Loading a Real-World Dataset\n",
    "\n",
    "We will use the **California Housing dataset**, which is available in scikit-learn. This dataset contains information about housing districts in California from the 1990 census. Our goal is to predict the median house value for a district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['MedHouseVal'] = housing.target\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Shape of the dataset:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f9c31",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's explore the dataset to understand its structure and the relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2271b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9373357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b356b748",
   "metadata": {},
   "source": [
    "### Visualizing the Data\n",
    "Let's create some visualizations to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc6a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the target variable (Median House Value)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['MedHouseVal'], bins=30, kde=True)\n",
    "plt.title('Distribution of Median House Value', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Median House Value ($100,000s)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c85504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Housing Features', fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6312e9",
   "metadata": {},
   "source": [
    "From the heatmap, we can see that `MedInc` (Median Income) has the strongest positive correlation with our target `MedHouseVal`. Let's visualize this relationship with a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20283d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of Median Income vs Median House Value\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='MedInc', y='MedHouseVal', alpha=0.4)\n",
    "plt.title('Median Income vs. Median House Value', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Median Income ($10,000s)')\n",
    "plt.ylabel('Median House Value ($100,000s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b82ee",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Before training our model, we need to prepare the data. This involves:\n",
    "1.  **Separating features (X) and target (y).**\n",
    "2.  **Splitting the data** into training and testing sets.\n",
    "3.  **Scaling the features** to ensure they are on a similar scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Separate features and target\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# 2. Split data into training and testing sets\n",
    "# We'll use 80% for training and 20% for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# 3. Scale the features\n",
    "# It's important to fit the scaler on the training data ONLY, then transform both train and test data.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2cdf3e",
   "metadata": {},
   "source": [
    "### Using `statsmodels` for more detailed analysis\n",
    "While `scikit-learn` is great for predictive modeling, the `statsmodels` library provides a more detailed statistical summary of the regression model, which is excellent for inferential analysis. Let's train a model with it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statsmodels requires us to add a constant to our features (for the intercept)\n",
    "X_train_sm = sm.add_constant(X_train_scaled)\n",
    "X_test_sm = sm.add_constant(X_test_scaled)\n",
    "\n",
    "# Create and train the model\n",
    "sm_model = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# Print the summary\n",
    "print(sm_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124873e",
   "metadata": {},
   "source": [
    "## 5. Training the Linear Regression Model\n",
    "\n",
    "Now we are ready to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e37aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Linear Regression model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e27b19",
   "metadata": {},
   "source": [
    "### Interpreting the Model\n",
    "Let's look at the intercept and coefficients of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the intercept\n",
    "print(f\"Intercept (β₀): {lin_reg.intercept_:.2f}\")\n",
    "\n",
    "# Create a DataFrame for the coefficients\n",
    "coeff_df = pd.DataFrame(lin_reg.coef_, X.columns, columns=['Coefficient'])\n",
    "print(\"\\nCoefficients (β₁...βₙ):\")\n",
    "print(coeff_df.sort_values('Coefficient', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74efcd6f",
   "metadata": {},
   "source": [
    "**Interpretation of Coefficients:**\n",
    "The coefficients tell us how a one-unit increase in a scaled feature affects the median house value, holding all other features constant. For example, a one-unit increase in scaled `MedInc` is associated with an increase of approximately $83,000 in the median house value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea10b36",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Now that we have a trained model, let's evaluate its performance on the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db96e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = lin_reg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc801c",
   "metadata": {},
   "source": [
    "We will use the following metrics to evaluate our model:\n",
    "-   **Mean Absolute Error (MAE):** The average of the absolute differences between predicted and actual values.\n",
    "-   **Mean Squared Error (MSE):** The average of the squared differences. Punishes larger errors more.\n",
    "-   **Root Mean Squared Error (RMSE):** The square root of MSE. It's in the same units as the target variable.\n",
    "-   **R-squared (R²):** The proportion of the variance in the dependent variable that is predictable from the independent variables. Ranges from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a9fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = metrics.r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daaa307",
   "metadata": {},
   "source": [
    "**Evaluation Interpretation:**\n",
    "-   **RMSE:** Our model's predictions are, on average, off by about $72,000.\n",
    "-   **R²:** Our model explains about 58% of the variability in median house values. This is a decent start but suggests there's room for improvement, perhaps with more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b3e6f",
   "metadata": {},
   "source": [
    "## 7. Visualizing Results\n",
    "\n",
    "Visualizing the model's performance can provide more insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f877d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of Actual vs. Predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.4)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2) # Perfect prediction line\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Median House Values\", fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c60f244",
   "metadata": {},
   "source": [
    "Ideally, the points should fall along the red dashed line. Our model follows the trend, but there's significant scatter, confirming our R² value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals Plot\n",
    "# Residuals are the difference between the actual and predicted values.\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, bins=30, kde=True)\n",
    "plt.title(\"Distribution of Residuals\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d30983",
   "metadata": {},
   "source": [
    "The residuals should be roughly normally distributed and centered around zero, which seems to be the case here. This indicates that our model's errors are random and not systematically biased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d0bee",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "In this session, we successfully built a Linear Regression model to predict California housing prices.\n",
    "\n",
    "✅ We understood the theory and assumptions of Linear Regression.\n",
    "✅ We loaded and explored a real-world dataset.\n",
    "✅ We performed essential preprocessing steps like train-test split and feature scaling.\n",
    "✅ We trained a model and interpreted its coefficients.\n",
    "✅ We evaluated the model's performance using key regression metrics (MAE, MSE, RMSE, R²).\n",
    "✅ We visualized the model's predictions and residuals.\n",
    "\n",
    "### What's Next?\n",
    "- **Improving the Model:** We could try polynomial regression to capture non-linear relationships or use more advanced models like Ridge or Lasso regression.\n",
    "- **Feature Engineering:** Creating new features from existing ones might improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc34feac",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "1.  **Change the `random_state`** in `train_test_split`. Does the model's performance change? Why?\n",
    "2.  **Train the model without feature scaling.** How do the evaluation metrics compare? Why is scaling important?\n",
    "3.  **Select only the top 3 correlated features** from the EDA step and train a new model. How does it perform compared to the model with all features?\n",
    "4.  **Interpret the coefficient for `AveRooms`.** What does it tell you about the relationship between the average number of rooms and the house value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192220c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your practice exercises\n",
    "\n",
    "# Exercise 3: Train on top 3 features\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f7a181",
   "metadata": {},
   "source": [
    "## Additional Examples with Different Datasets\n",
    "\n",
    "To solidify our understanding, let's apply linear regression to a few more real-world datasets.\n",
    "\n",
    "### Example 1: Predicting Medical Insurance Costs\n",
    "\n",
    "In this example, we'll use a dataset of medical insurance costs to predict a person's medical expenses based on their age, sex, BMI, number of children, and whether they are a smoker.\n",
    "\n",
    "**Dataset Source:** [Medical Cost Personal Datasets on Kaggle](https://www.kaggle.com/datasets/mirichoi0218/insurance) (originally from GitHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3836b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a public GitHub repository\n",
    "url = 'https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv'\n",
    "insurance_df = pd.read_csv(url)\n",
    "\n",
    "# --- 1. Preprocessing ---\n",
    "# Convert categorical variables to numerical\n",
    "insurance_df_processed = pd.get_dummies(insurance_df, columns=['sex', 'smoker', 'region'], drop_first=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_ins = insurance_df_processed.drop('charges', axis=1)\n",
    "y_ins = insurance_df_processed['charges']\n",
    "\n",
    "# Split data\n",
    "X_train_ins, X_test_ins, y_train_ins, y_test_ins = train_test_split(X_ins, y_ins, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler_ins = StandardScaler()\n",
    "X_train_ins_scaled = scaler_ins.fit_transform(X_train_ins)\n",
    "X_test_ins_scaled = scaler_ins.transform(X_test_ins)\n",
    "\n",
    "# --- 2. Model Training ---\n",
    "ins_model = LinearRegression()\n",
    "ins_model.fit(X_train_ins_scaled, y_train_ins)\n",
    "\n",
    "# --- 3. Evaluation ---\n",
    "y_pred_ins = ins_model.predict(X_test_ins_scaled)\n",
    "r2_ins = metrics.r2_score(y_test_ins, y_pred_ins)\n",
    "rmse_ins = np.sqrt(metrics.mean_squared_error(y_test_ins, y_pred_ins))\n",
    "\n",
    "print(\"--- Medical Insurance Cost Prediction ---\")\n",
    "print(f\"Model R-squared: {r2_ins:.2f}\")\n",
    "print(f\"Model RMSE: ${rmse_ins:,.2f}\")\n",
    "\n",
    "# --- 4. Visualization ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_ins, y_pred_ins, alpha=0.5)\n",
    "plt.plot([y_test_ins.min(), y_test_ins.max()], [y_test_ins.min(), y_test_ins.max()], 'r--', lw=2)\n",
    "plt.title('Actual vs. Predicted Insurance Charges', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Actual Charges ($)')\n",
    "plt.ylabel('Predicted Charges ($)')\n",
    "plt.show()\n",
    "\n",
    "# --- 5. Coefficient Analysis ---\n",
    "coeffs = pd.DataFrame(ins_model.coef_, index=X_ins.columns, columns=['Coefficient'])\n",
    "print(\"\\nCoefficients:\")\n",
    "print(coeffs.sort_values('Coefficient', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d149e",
   "metadata": {},
   "source": [
    "### Example 2: Predicting Fish Weight\n",
    "\n",
    "Here, we'll predict the weight of a fish based on its physical measurements (length, height, width) and species. This is a classic regression problem.\n",
    "\n",
    "**Dataset Source:** [Fish Market on Kaggle](https://www.kaggle.com/datasets/aungpyaeap/fish-market) (originally from GitHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/marcopeix/datasciencewithmarco/master/data/Fish.csv'\n",
    "fish_df = pd.read_csv(url)\n",
    "\n",
    "# --- 1. Preprocessing ---\n",
    "# Convert categorical species to numerical\n",
    "fish_df_processed = pd.get_dummies(fish_df, columns=['Species'], drop_first=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_fish = fish_df_processed.drop('Weight', axis=1)\n",
    "y_fish = fish_df_processed['Weight']\n",
    "\n",
    "# Split data\n",
    "X_train_fish, X_test_fish, y_train_fish, y_test_fish = train_test_split(X_fish, y_fish, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler_fish = StandardScaler()\n",
    "X_train_fish_scaled = scaler_fish.fit_transform(X_train_fish)\n",
    "X_test_fish_scaled = scaler_fish.transform(X_test_fish)\n",
    "\n",
    "# --- 2. Model Training ---\n",
    "fish_model = LinearRegression()\n",
    "fish_model.fit(X_train_fish_scaled, y_train_fish)\n",
    "\n",
    "# --- 3. Evaluation ---\n",
    "y_pred_fish = fish_model.predict(X_test_fish_scaled)\n",
    "r2_fish = metrics.r2_score(y_test_fish, y_pred_fish)\n",
    "rmse_fish = np.sqrt(metrics.mean_squared_error(y_test_fish, y_pred_fish))\n",
    "\n",
    "print(\"--- Fish Weight Prediction ---\")\n",
    "print(f\"Model R-squared: {r2_fish:.2f}\")\n",
    "print(f\"Model RMSE: {rmse_fish:.2f} grams\")\n",
    "\n",
    "# --- 4. Visualization ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_fish, y_pred_fish, alpha=0.6)\n",
    "plt.plot([y_test_fish.min(), y_test_fish.max()], [y_test_fish.min(), y_test_fish.max()], 'r--', lw=2)\n",
    "plt.title('Actual vs. Predicted Fish Weight', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Actual Weight (g)')\n",
    "plt.ylabel('Predicted Weight (g)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e46b2",
   "metadata": {},
   "source": [
    "### Example 3: Predicting Car Prices\n",
    "\n",
    "In this final example, we'll predict the price of a car based on various features like its make, model, year, engine size, and horsepower.\n",
    "\n",
    "**Dataset Source:** [Car Price Prediction on Kaggle](https://www.kaggle.com/datasets/hellbuoy/car-price-prediction) (originally from GitHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b715b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/hellbuoy/car-price-prediction/master/CarPrice_Assignment.csv'\n",
    "car_df = pd.read_csv(url)\n",
    "\n",
    "# --- 1. Preprocessing ---\n",
    "# Select only numeric columns for simplicity\n",
    "car_numeric = car_df.select_dtypes(include=np.number)\n",
    "car_numeric = car_numeric.drop('car_ID', axis=1) # Drop irrelevant ID\n",
    "car_numeric.dropna(inplace=True) # Drop rows with missing values\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_car = car_numeric.drop('price', axis=1)\n",
    "y_car = car_numeric['price']\n",
    "\n",
    "# Split data\n",
    "X_train_car, X_test_car, y_train_car, y_test_car = train_test_split(X_car, y_car, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler_car = StandardScaler()\n",
    "X_train_car_scaled = scaler_car.fit_transform(X_train_car)\n",
    "X_test_car_scaled = scaler_car.transform(X_test_car)\n",
    "\n",
    "# --- 2. Model Training ---\n",
    "car_model = LinearRegression()\n",
    "car_model.fit(X_train_car_scaled, y_train_car)\n",
    "\n",
    "# --- 3. Evaluation ---\n",
    "y_pred_car = car_model.predict(X_test_car_scaled)\n",
    "r2_car = metrics.r2_score(y_test_car, y_pred_car)\n",
    "rmse_car = np.sqrt(metrics.mean_squared_error(y_test_car, y_pred_car))\n",
    "\n",
    "print(\"--- Car Price Prediction ---\")\n",
    "print(f\"Model R-squared: {r2_car:.2f}\")\n",
    "print(f\"Model RMSE: ${rmse_car:,.2f}\")\n",
    "\n",
    "# --- 4. Visualization ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_car, y_pred_car, alpha=0.6)\n",
    "plt.plot([y_test_car.min(), y_test_car.max()], [y_test_car.min(), y_test_car.max()], 'r--', lw=2)\n",
    "plt.title('Actual vs. Predicted Car Prices', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Actual Price ($)')\n",
    "plt.ylabel('Predicted Price ($)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
