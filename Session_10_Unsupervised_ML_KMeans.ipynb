{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8d99f1",
   "metadata": {},
   "source": [
    "# Session 10: KNN and Unsupervised Machine Learning\n",
    "\n",
    "## Overview\n",
    "This session covers both K-Nearest Neighbors (KNN) - a supervised learning algorithm, and Unsupervised Machine Learning focusing on K-Means clustering. We'll explore how both algorithms use the concept of \"k\" but for different purposes.\n",
    "\n",
    "### Topics Covered:\n",
    "1. **K-Nearest Neighbors (KNN) Algorithm**\n",
    "   - Classification and regression\n",
    "   - Choosing optimal k\n",
    "   - Distance metrics and scaling\n",
    "2. **Introduction to Unsupervised Learning**\n",
    "3. **K-Means Clustering Algorithm**\n",
    "4. **Determining Optimal Number of Clusters (K)**\n",
    "   - Elbow Method\n",
    "   - Silhouette Analysis\n",
    "5. **Comparison: KNN vs K-Means**\n",
    "6. **Practical Implementation and Evaluation**\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand KNN algorithm for supervised learning\n",
    "- Learn principles of unsupervised learning\n",
    "- Master K-Means clustering algorithm\n",
    "- Compare and contrast KNN and K-Means\n",
    "- Apply optimal k selection techniques for both algorithms\n",
    "- Evaluate model performance and clustering quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7430976",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN) Algorithm\n",
    "\n",
    "### What is KNN?\n",
    "K-Nearest Neighbors is a **supervised learning** algorithm that can be used for both classification and regression. It's called \"lazy learning\" because it doesn't build an explicit model during training - instead, it stores all training data and makes predictions based on the k nearest neighbors.\n",
    "\n",
    "### How KNN Works:\n",
    "1. **Store** all training data points\n",
    "2. **Calculate distance** from new point to all training points\n",
    "3. **Find k nearest** neighbors\n",
    "4. **Make prediction**:\n",
    "   - **Classification**: Majority vote of k neighbors\n",
    "   - **Regression**: Average of k neighbors' values\n",
    "\n",
    "### Key Concepts:\n",
    "- **k**: Number of neighbors to consider\n",
    "- **Distance metric**: Usually Euclidean distance\n",
    "- **Voting**: How neighbors influence the prediction\n",
    "- **Curse of dimensionality**: Performance degrades in high dimensions\n",
    "\n",
    "### Applications:\n",
    "- Recommendation systems\n",
    "- Pattern recognition\n",
    "- Image classification\n",
    "- Text mining\n",
    "- Market research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508a0e9",
   "metadata": {},
   "source": [
    "### KNN Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276fbb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic classification dataset\n",
    "X_class, y_class = make_classification(n_samples=300, n_features=2, n_redundant=0, \n",
    "                                      n_informative=2, n_clusters_per_class=1, \n",
    "                                      random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_class, y_class, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_class))}\")\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_class[:, 0], X_class[:, 1], c=y_class, cmap='viridis', alpha=0.7)\n",
    "plt.title('Synthetic Classification Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(scatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8926c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different values of k\n",
    "k_values = [1, 3, 5, 7, 9, 15]\n",
    "accuracies = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, k in enumerate(k_values):\n",
    "    # Create and train KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Create decision boundary visualization\n",
    "    h = 0.02  # step size in the mesh\n",
    "    x_min, x_max = X_class[:, 0].min() - 1, X_class[:, 0].max() + 1\n",
    "    y_min, y_max = X_class[:, 1].min() - 1, X_class[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict on mesh points\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    axes[i].contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\n",
    "    scatter = axes[i].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', \n",
    "                             edgecolors='black')\n",
    "    axes[i].set_title(f'KNN (k={k}), Accuracy: {accuracy:.3f}')\n",
    "    axes[i].set_xlabel('Feature 1')\n",
    "    axes[i].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy vs k\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN Classification: Accuracy vs k')\n",
    "plt.grid(True, alpha=0.3)\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "plt.axvline(x=best_k, color='red', linestyle='--', alpha=0.7, \n",
    "           label=f'Best k = {best_k}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best k value: {best_k} with accuracy: {max(accuracies):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c0a7c7",
   "metadata": {},
   "source": [
    "### KNN on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75264cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris_full = iris.data\n",
    "y_iris_full = iris.target\n",
    "\n",
    "# Split the data\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(\n",
    "    X_iris_full, y_iris_full, test_size=0.3, random_state=42, stratify=y_iris_full\n",
    ")\n",
    "\n",
    "# Scale the features (important for KNN)\n",
    "scaler = StandardScaler()\n",
    "X_iris_train_scaled = scaler.fit_transform(X_iris_train)\n",
    "X_iris_test_scaled = scaler.transform(X_iris_test)\n",
    "\n",
    "# Test different k values for Iris\n",
    "k_range = range(1, 16)\n",
    "iris_accuracies = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn_iris = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_iris.fit(X_iris_train_scaled, y_iris_train)\n",
    "    y_iris_pred = knn_iris.predict(X_iris_test_scaled)\n",
    "    accuracy = accuracy_score(y_iris_test, y_iris_pred)\n",
    "    iris_accuracies.append(accuracy)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, iris_accuracies, 'go-', linewidth=2, markersize=6)\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN on Iris Dataset: Accuracy vs k')\n",
    "plt.grid(True, alpha=0.3)\n",
    "best_k_iris = k_range[np.argmax(iris_accuracies)]\n",
    "plt.axvline(x=best_k_iris, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Use best k for final model\n",
    "knn_best = KNeighborsClassifier(n_neighbors=best_k_iris)\n",
    "knn_best.fit(X_iris_train_scaled, y_iris_train)\n",
    "y_iris_final_pred = knn_best.predict(X_iris_test_scaled)\n",
    "\n",
    "# Confusion matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "cm = confusion_matrix(y_iris_test, y_iris_final_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.title(f'Confusion Matrix (k={best_k_iris})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best k for Iris: {best_k_iris}\")\n",
    "print(f\"Best accuracy: {max(iris_accuracies):.3f}\")\n",
    "print(f\"\\\\nClassification Report:\")\n",
    "print(classification_report(y_iris_test, y_iris_final_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b6032b",
   "metadata": {},
   "source": [
    "### Choosing Optimal k in KNN\n",
    "\n",
    "#### Key Considerations:\n",
    "1. **Odd vs Even k**: Use odd k for binary classification to avoid ties\n",
    "2. **Small k**: More sensitive to local patterns (high variance, low bias)\n",
    "3. **Large k**: More stable but may lose local patterns (low variance, high bias)\n",
    "4. **Rule of thumb**: k = √n (where n is number of training samples)\n",
    "\n",
    "#### Cross-Validation Approach:\n",
    "Use cross-validation to find optimal k by testing different values and choosing the one with best average performance.\n",
    "\n",
    "### KNN vs K-Means: Key Differences\n",
    "\n",
    "| Aspect | KNN | K-Means |\n",
    "|--------|-----|---------|\n",
    "| **Learning Type** | Supervised | Unsupervised |\n",
    "| **Purpose** | Classification/Regression | Clustering |\n",
    "| **Training** | Lazy (no training phase) | Eager (builds model) |\n",
    "| **Prediction** | Based on k nearest neighbors | Assigns to nearest centroid |\n",
    "| **k meaning** | Number of neighbors | Number of clusters |\n",
    "| **Distance** | To find neighbors | To assign clusters |\n",
    "| **Output** | Class label or value | Cluster assignment |\n",
    "\n",
    "### Important Notes:\n",
    "- **Feature Scaling**: Always scale features for KNN (distance-based)\n",
    "- **Curse of Dimensionality**: Performance degrades in high dimensions\n",
    "- **Computational Cost**: KNN is expensive at prediction time\n",
    "- **Memory Usage**: Stores all training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf25d8",
   "metadata": {},
   "source": [
    "## 1. Introduction to Unsupervised Learning\n",
    "\n",
    "### What is Unsupervised Learning?\n",
    "Unsupervised learning algorithms find patterns in data without being given explicit target labels. Main types include:\n",
    "\n",
    "1. **Clustering**: Grouping similar data points together\n",
    "2. **Dimensionality Reduction**: Reducing the number of features while preserving information\n",
    "3. **Association Rules**: Finding relationships between different variables\n",
    "4. **Anomaly Detection**: Identifying unusual data points\n",
    "\n",
    "### Applications:\n",
    "- Customer segmentation\n",
    "- Market research\n",
    "- Image segmentation\n",
    "- Gene sequencing\n",
    "- Recommendation systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f2bf582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs, load_iris, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346ea4a",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset with known clusters for demonstration\n",
    "X_synthetic, y_true = make_blobs(n_samples=300, centers=4, n_features=2, \n",
    "                                random_state=42, cluster_std=1.5)\n",
    "\n",
    "print(f\"Synthetic dataset shape: {X_synthetic.shape}\")\n",
    "print(f\"Number of true clusters: {len(np.unique(y_true))}\")\n",
    "\n",
    "# Also load the Iris dataset for real-world example\n",
    "iris = load_iris()\n",
    "X_iris = iris.data[:, :2]  # Use only first 2 features for visualization\n",
    "y_iris_true = iris.target\n",
    "\n",
    "print(f\"\\nIris dataset shape: {X_iris.shape}\")\n",
    "print(f\"Iris features used: {iris.feature_names[:2]}\")\n",
    "print(f\"Number of true species: {len(np.unique(y_iris_true))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2315b1c8",
   "metadata": {},
   "source": [
    "## 3. Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e778fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Synthetic data\n",
    "scatter1 = axes[0].scatter(X_synthetic[:, 0], X_synthetic[:, 1], c=y_true, \n",
    "                          cmap='viridis', alpha=0.7)\n",
    "axes[0].set_title('Synthetic Dataset (True Clusters)')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter1, ax=axes[0])\n",
    "\n",
    "# Iris data\n",
    "scatter2 = axes[1].scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris_true, \n",
    "                          cmap='viridis', alpha=0.7)\n",
    "axes[1].set_title('Iris Dataset (True Species)')\n",
    "axes[1].set_xlabel('Sepal Length (cm)')\n",
    "axes[1].set_ylabel('Sepal Width (cm)')\n",
    "plt.colorbar(scatter2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e426a482",
   "metadata": {},
   "source": [
    "## 4. K-Means Clustering Algorithm\n",
    "\n",
    "### How K-Means Works:\n",
    "1. **Initialize**: Choose K cluster centers randomly\n",
    "2. **Assign**: Assign each point to the nearest cluster center\n",
    "3. **Update**: Move cluster centers to the mean of assigned points\n",
    "4. **Repeat**: Steps 2-3 until convergence\n",
    "\n",
    "### Key Parameters:\n",
    "- **n_clusters (k)**: Number of clusters\n",
    "- **init**: Method for initialization ('k-means++' is default)\n",
    "- **max_iter**: Maximum number of iterations\n",
    "- **random_state**: For reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca20725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement basic K-Means clustering with a fixed K\n",
    "def apply_kmeans(X, k, title):\n",
    "    \"\"\"Apply K-Means clustering and visualize results\"\"\"\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # Calculate metrics\n",
    "    inertia = kmeans.inertia_  # Within-cluster sum of squares\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, linewidths=3)\n",
    "    plt.title(f'{title} - K={k}\\nInertia: {inertia:.2f}, Silhouette Score: {silhouette_avg:.3f}')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.show()\n",
    "    \n",
    "    return kmeans, inertia, silhouette_avg\n",
    "\n",
    "# Apply K-Means with K=4 to synthetic data\n",
    "print(\"K-Means Clustering on Synthetic Data:\")\n",
    "kmeans_synthetic, _, _ = apply_kmeans(X_synthetic, 4, \"Synthetic Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9734218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with K=3 to Iris data\n",
    "print(\"K-Means Clustering on Iris Data:\")\n",
    "kmeans_iris, _, _ = apply_kmeans(X_iris, 3, \"Iris Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b577436",
   "metadata": {},
   "source": [
    "## 5. Elbow Method for Optimal K\n",
    "\n",
    "### What is the Elbow Method?\n",
    "The elbow method plots the Within-Cluster Sum of Squares (WCSS) against different values of K. The \"elbow\" point where the rate of decrease sharply changes indicates the optimal K.\n",
    "\n",
    "### WCSS (Inertia):\n",
    "Sum of squared distances from each point to its cluster centroid. Lower values indicate tighter clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0856f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(X, max_k=10, title=\"Elbow Method\"):\n",
    "    \"\"\"Apply elbow method to find optimal K\"\"\"\n",
    "    k_range = range(1, max_k + 1)\n",
    "    inertias = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plot the elbow curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "    plt.title(f'{title} - Elbow Method')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate and plot the rate of change\n",
    "    if len(inertias) > 2:\n",
    "        differences = np.diff(inertias)\n",
    "        second_differences = np.diff(differences)\n",
    "        elbow_point = np.argmax(second_differences) + 2  # +2 because of double differencing\n",
    "        plt.axvline(x=elbow_point, color='red', linestyle='--', alpha=0.7, \n",
    "                   label=f'Suggested K = {elbow_point}')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return k_range, inertias\n",
    "\n",
    "# Apply elbow method to synthetic data\n",
    "print(\"Elbow Method Analysis for Synthetic Data:\")\n",
    "k_range_syn, inertias_syn = elbow_method(X_synthetic, max_k=10, title=\"Synthetic Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply elbow method to Iris data\n",
    "print(\"Elbow Method Analysis for Iris Data:\")\n",
    "k_range_iris, inertias_iris = elbow_method(X_iris, max_k=8, title=\"Iris Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df8c2b1",
   "metadata": {},
   "source": [
    "## 6. Silhouette Analysis for K Selection\n",
    "\n",
    "### What is Silhouette Analysis?\n",
    "Silhouette analysis measures how similar a point is to its own cluster compared to other clusters.\n",
    "\n",
    "### Silhouette Score:\n",
    "- **Range**: -1 to +1\n",
    "- **+1**: Perfect clustering\n",
    "- **0**: Point is on the border between clusters\n",
    "- **-1**: Point might be assigned to wrong cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_analysis(X, max_k=10, title=\"Silhouette Analysis\"):\n",
    "    \"\"\"Perform silhouette analysis to find optimal K\"\"\"\n",
    "    k_range = range(2, max_k + 1)  # Silhouette score needs at least 2 clusters\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "    # Plot silhouette scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('Average Silhouette Score')\n",
    "    plt.title(f'{title} - Silhouette Analysis')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark the best K\n",
    "    best_k = k_range[np.argmax(silhouette_scores)]\n",
    "    best_score = max(silhouette_scores)\n",
    "    plt.axvline(x=best_k, color='red', linestyle='--', alpha=0.7, \n",
    "               label=f'Best K = {best_k} (Score: {best_score:.3f})')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return k_range, silhouette_scores, best_k\n",
    "\n",
    "# Apply silhouette analysis to synthetic data\n",
    "print(\"Silhouette Analysis for Synthetic Data:\")\n",
    "k_range_sil_syn, sil_scores_syn, best_k_syn = silhouette_analysis(X_synthetic, max_k=10, \n",
    "                                                                  title=\"Synthetic Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d87418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply silhouette analysis to Iris data\n",
    "print(\"Silhouette Analysis for Iris Data:\")\n",
    "k_range_sil_iris, sil_scores_iris, best_k_iris = silhouette_analysis(X_iris, max_k=8, \n",
    "                                                                     title=\"Iris Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d983205",
   "metadata": {},
   "source": [
    "## 8. Apply K-Means with Optimal K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fd698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of optimal K values from different methods\n",
    "print(\"=== OPTIMAL K SELECTION SUMMARY ===\\n\")\n",
    "\n",
    "print(\"Synthetic Dataset:\")\n",
    "print(f\"True number of clusters: 4\")\n",
    "print(f\"Silhouette Analysis suggests K = {best_k_syn}\")\n",
    "print(\"Elbow Method: Look at the plot above for the 'elbow' point\\n\")\n",
    "\n",
    "print(\"Iris Dataset:\")\n",
    "print(f\"True number of species: 3\")\n",
    "print(f\"Silhouette Analysis suggests K = {best_k_iris}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0965f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with the optimal K values\n",
    "def final_clustering_analysis(X, optimal_k, true_labels, title):\n",
    "    \"\"\"Perform final clustering with optimal K and compare with ground truth\"\"\"\n",
    "    # Apply K-Means with optimal K\n",
    "    kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    predicted_labels = kmeans_optimal.fit_predict(X)\n",
    "    centroids = kmeans_optimal.cluster_centers_\n",
    "    \n",
    "    # Calculate metrics\n",
    "    inertia = kmeans_optimal.inertia_\n",
    "    silhouette_avg = silhouette_score(X, predicted_labels)\n",
    "    \n",
    "    # Create visualization comparing true vs predicted clusters\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # True clusters\n",
    "    scatter1 = axes[0].scatter(X[:, 0], X[:, 1], c=true_labels, cmap='viridis', alpha=0.7)\n",
    "    axes[0].set_title(f'{title} - True Clusters')\n",
    "    axes[0].set_xlabel('Feature 1')\n",
    "    axes[0].set_ylabel('Feature 2')\n",
    "    \n",
    "    # Predicted clusters\n",
    "    scatter2 = axes[1].scatter(X[:, 0], X[:, 1], c=predicted_labels, cmap='viridis', alpha=0.7)\n",
    "    axes[1].scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, linewidths=3)\n",
    "    axes[1].set_title(f'{title} - K-Means (K={optimal_k})\\nSilhouette Score: {silhouette_avg:.3f}')\n",
    "    axes[1].set_xlabel('Feature 1')\n",
    "    axes[1].set_ylabel('Feature 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return kmeans_optimal, predicted_labels\n",
    "\n",
    "# Final clustering for synthetic data\n",
    "print(\"Final Clustering Results:\\n\")\n",
    "kmeans_final_syn, pred_syn = final_clustering_analysis(X_synthetic, best_k_syn, y_true, \n",
    "                                                       \"Synthetic Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e97b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final clustering for Iris data\n",
    "kmeans_final_iris, pred_iris = final_clustering_analysis(X_iris, best_k_iris, y_iris_true, \n",
    "                                                         \"Iris Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255247b",
   "metadata": {},
   "source": [
    "## 9. Comparison of K Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Elbow Method - Synthetic\n",
    "axes[0, 0].plot(k_range_syn, inertias_syn, 'bo-', linewidth=2, markersize=6)\n",
    "axes[0, 0].set_title('Elbow Method - Synthetic Data')\n",
    "axes[0, 0].set_xlabel('Number of Clusters (K)')\n",
    "axes[0, 0].set_ylabel('WCSS')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette Analysis - Synthetic\n",
    "axes[0, 1].plot(k_range_sil_syn, sil_scores_syn, 'go-', linewidth=2, markersize=6)\n",
    "axes[0, 1].axvline(x=best_k_syn, color='red', linestyle='--', alpha=0.7)\n",
    "axes[0, 1].set_title('Silhouette Analysis - Synthetic Data')\n",
    "axes[0, 1].set_xlabel('Number of Clusters (K)')\n",
    "axes[0, 1].set_ylabel('Silhouette Score')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Elbow Method - Iris\n",
    "axes[1, 0].plot(k_range_iris, inertias_iris, 'bo-', linewidth=2, markersize=6)\n",
    "axes[1, 0].set_title('Elbow Method - Iris Data')\n",
    "axes[1, 0].set_xlabel('Number of Clusters (K)')\n",
    "axes[1, 0].set_ylabel('WCSS')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette Analysis - Iris\n",
    "axes[1, 1].plot(k_range_sil_iris, sil_scores_iris, 'go-', linewidth=2, markersize=6)\n",
    "axes[1, 1].axvline(x=best_k_iris, color='red', linestyle='--', alpha=0.7)\n",
    "axes[1, 1].set_title('Silhouette Analysis - Iris Data')\n",
    "axes[1, 1].set_xlabel('Number of Clusters (K)')\n",
    "axes[1, 1].set_ylabel('Silhouette Score')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323d441",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways and Best Practices\n",
    "\n",
    "### Method Comparison:\n",
    "\n",
    "1. **Elbow Method**:\n",
    "   - **Pros**: Simple to understand and implement\n",
    "   - **Cons**: \"Elbow\" point can be subjective\n",
    "   - **Best for**: Quick initial assessment\n",
    "\n",
    "2. **Silhouette Analysis**:\n",
    "   - **Pros**: Considers both cohesion and separation\n",
    "   - **Cons**: Can be computationally expensive\n",
    "   - **Best for**: Balanced cluster evaluation\n",
    "\n",
    "### Best Practices:\n",
    "- Use multiple methods and compare results\n",
    "- Consider domain knowledge about expected clusters\n",
    "- Standardize features if they have different scales\n",
    "- Validate results with business/domain expertise\n",
    "- Consider the interpretability of the resulting clusters\n",
    "\n",
    "### When to Use K-Means:\n",
    "- ✅ Spherical clusters of similar size\n",
    "- ✅ Clear separation between clusters\n",
    "- ✅ Numerical features\n",
    "- ❌ Non-spherical clusters\n",
    "- ❌ Clusters of very different sizes\n",
    "- ❌ High-dimensional data without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=== SESSION 10 SUMMARY ===\\n\")\n",
    "print(\"📚 What we learned:\")\n",
    "print(\"1. Unsupervised learning finds patterns without labeled data\")\n",
    "print(\"2. K-Means clustering groups similar data points\")\n",
    "print(\"3. Multiple methods exist to determine optimal K:\")\n",
    "print(\"   - Elbow Method (WCSS analysis)\")\n",
    "print(\"   - Silhouette Analysis (cluster quality)\")\n",
    "print(\"4. Each method has strengths and limitations\")\n",
    "print(\"5. Combining multiple methods gives more robust results\")\n",
    "\n",
    "print(\"\\n🎯 Results on our datasets:\")\n",
    "print(f\"Synthetic Data (True K=4): Silhouette method suggested K={best_k_syn}\")\n",
    "print(f\"Iris Data (True K=3): Silhouette method suggested K={best_k_iris}\")\n",
    "\n",
    "print(\"\\n🚀 Next steps:\")\n",
    "print(\"- Try K-Means on your own datasets\")\n",
    "print(\"- Experiment with other clustering algorithms (Hierarchical, DBSCAN)\")\n",
    "print(\"- Learn about dimensionality reduction (PCA, t-SNE)\")\n",
    "print(\"- Explore clustering evaluation metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
