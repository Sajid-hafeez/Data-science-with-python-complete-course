{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2cca9c8",
   "metadata": {},
   "source": [
    "# Session 7: Logistic Regression with Heart Disease Dataset\n",
    "\n",
    "## Course: Data Science with Python\n",
    "### Date: August 29, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will be able to:\n",
    "1. Understand the fundamentals of logistic regression\n",
    "2. Implement logistic regression on real-world data\n",
    "3. Evaluate classification model performance\n",
    "4. Interpret model results and feature importance\n",
    "\n",
    "## What is Logistic Regression?\n",
    "Logistic regression is a statistical method used for **binary classification** problems. Unlike linear regression which predicts continuous values, logistic regression predicts the probability that an instance belongs to a particular category.\n",
    "\n",
    "### Key Differences from Linear Regression:\n",
    "- **Output**: Probabilities (0 to 1) instead of continuous values\n",
    "- **Function**: Uses sigmoid/logistic function instead of linear function\n",
    "- **Purpose**: Classification instead of regression\n",
    "\n",
    "### The Sigmoid Function:\n",
    "The logistic function maps any real number to a value between 0 and 1:\n",
    "$$P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ec52b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Let's start by importing all the necessary libraries for our logistic regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8eab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Import warnings to suppress unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up matplotlib for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438b0e84",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Heart Disease Dataset\n",
    "\n",
    "### About the Dataset\n",
    "We'll be working with the **Heart Disease Dataset**, which contains medical data that can help predict whether a patient has heart disease or not. This is a classic binary classification problem perfect for logistic regression.\n",
    "\n",
    "**Objective**: Predict whether a patient has heart disease (1) or not (0) based on various medical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc037d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the heart disease dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33301f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 50)\n",
    "df.info()\n",
    "\n",
    "print(\"\\nDataset Description:\")\n",
    "print(\"=\" * 50)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6f19e",
   "metadata": {},
   "source": [
    "## 3. Introduction to Data Variables\n",
    "\n",
    "Understanding each variable in our dataset is crucial for building an effective model. Let's examine each feature:\n",
    "\n",
    "### Feature Variables (Input Features):\n",
    "\n",
    "| Variable | Description | Type | Values |\n",
    "|----------|-------------|------|--------|\n",
    "| **age** | Age of the patient | Continuous | 29-77 years |\n",
    "| **sex** | Gender of the patient | Categorical | 1 = Male, 0 = Female |\n",
    "| **cp** | Chest pain type | Categorical | 0 = Typical angina<br>1 = Atypical angina<br>2 = Non-anginal pain<br>3 = Asymptomatic |\n",
    "| **trestbps** | Resting blood pressure | Continuous | mm Hg |\n",
    "| **chol** | Serum cholesterol | Continuous | mg/dl |\n",
    "| **fbs** | Fasting blood sugar > 120 mg/dl | Categorical | 1 = True, 0 = False |\n",
    "| **restecg** | Resting electrocardiographic results | Categorical | 0 = Normal<br>1 = ST-T wave abnormality<br>2 = Left ventricular hypertrophy |\n",
    "| **thalach** | Maximum heart rate achieved | Continuous | beats per minute |\n",
    "| **exang** | Exercise induced angina | Categorical | 1 = Yes, 0 = No |\n",
    "| **oldpeak** | ST depression induced by exercise | Continuous | 0-6.2 |\n",
    "| **slope** | Slope of peak exercise ST segment | Categorical | 0 = Upsloping<br>1 = Flat<br>2 = Downsloping |\n",
    "| **ca** | Number of major vessels colored by fluoroscopy | Discrete | 0-4 |\n",
    "| **thal** | Thalassemia | Categorical | 1 = Normal<br>2 = Fixed defect<br>3 = Reversible defect |\n",
    "\n",
    "### Target Variable (Output):\n",
    "| Variable | Description | Type | Values |\n",
    "|----------|-------------|------|--------|\n",
    "| **target** | Heart disease diagnosis | Binary | 1 = Heart disease present<br>0 = No heart disease |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of our target variable\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(\"=\" * 40)\n",
    "target_counts = df['target'].value_counts()\n",
    "print(target_counts)\n",
    "print(f\"\\nPercentage distribution:\")\n",
    "print(f\"No Heart Disease (0): {target_counts[0]/len(df)*100:.1f}%\")\n",
    "print(f\"Heart Disease (1): {target_counts[1]/len(df)*100:.1f}%\")\n",
    "\n",
    "# Visualize the target distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "df['target'].value_counts().plot(kind='bar', color=['lightcoral', 'lightblue'])\n",
    "plt.title('Heart Disease Distribution')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['No Disease', 'Disease'], rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['target'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['lightcoral', 'lightblue'])\n",
    "plt.title('Heart Disease Percentage')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5452a46",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Before building our logistic regression model, we need to prepare our data:\n",
    "1. Check for missing values\n",
    "2. Handle any data quality issues\n",
    "3. Prepare features and target variables\n",
    "4. Scale features if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Check:\")\n",
    "print(\"=\" * 30)\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"\\n✓ Great! No missing values found in the dataset.\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Found {missing_values.sum()} missing values that need to be handled.\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)  # Features (all columns except target)\n",
    "y = df['target']               # Target variable\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e39b61",
   "metadata": {},
   "source": [
    "## 5. Split Data into Training and Test Sets\n",
    "\n",
    "We'll split our data into training and testing sets to evaluate our model's performance on unseen data.\n",
    "\n",
    "**Why do we split the data?**\n",
    "- **Training set**: Used to train the model (learn patterns)\n",
    "- **Test set**: Used to evaluate model performance (unseen data)\n",
    "- This helps us detect overfitting and get realistic performance estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dec9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "# 80% for training, 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,        # 20% for testing\n",
    "    random_state=42,      # For reproducible results\n",
    "    stratify=y            # Maintain same proportion of target classes in both sets\n",
    ")\n",
    "\n",
    "print(\"Data Split Summary:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Training samples: {len(X_train)} ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"Testing samples: {len(X_test)} ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Check target distribution in both sets\n",
    "print(f\"\\nTarget distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(f\"\\nTarget distribution in testing set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50615030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "# Logistic regression can benefit from feature scaling, especially when features have different scales\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data and transform both training and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature Scaling Applied:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"✓ Features have been standardized (mean=0, std=1)\")\n",
    "print(f\"Original training data range: {X_train.min().min():.2f} to {X_train.max().max():.2f}\")\n",
    "print(f\"Scaled training data range: {X_train_scaled.min():.2f} to {X_train_scaled.max():.2f}\")\n",
    "\n",
    "# Convert back to DataFrame for easier handling (optional)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265bf36b",
   "metadata": {},
   "source": [
    "## 6. Implement Logistic Regression Model\n",
    "\n",
    "Now we'll create and train our logistic regression model. Scikit-learn makes this process straightforward.\n",
    "\n",
    "### Key Parameters of LogisticRegression:\n",
    "- **random_state**: For reproducible results\n",
    "- **max_iter**: Maximum number of iterations for optimization\n",
    "- **solver**: Algorithm for optimization ('liblinear' works well for small datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fc06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the logistic regression model\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,      # For reproducible results\n",
    "    max_iter=1000,        # Increase if convergence issues occur\n",
    "    solver='liblinear'    # Good solver for smaller datasets\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Logistic Regression Model...\")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "print(\"✓ Model training completed!\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = lr_model.predict(X_train_scaled)\n",
    "y_test_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_train_proba = lr_model.predict_proba(X_train_scaled)[:, 1]  # Probability of class 1\n",
    "y_test_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\nModel Predictions Generated:\")\n",
    "print(f\"Training predictions shape: {y_train_pred.shape}\")\n",
    "print(f\"Test predictions shape: {y_test_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model coefficients\n",
    "print(\"Model Coefficients Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "coefficients_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lr_model.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(lr_model.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (based on coefficient magnitude):\")\n",
    "print(coefficients_df)\n",
    "\n",
    "print(f\"\\nIntercept (bias term): {lr_model.intercept_[0]:.4f}\")\n",
    "\n",
    "# Interpretation note\n",
    "print(\"\\nCoefficient Interpretation:\")\n",
    "print(\"• Positive coefficient: Increases probability of heart disease\")\n",
    "print(\"• Negative coefficient: Decreases probability of heart disease\")\n",
    "print(\"• Larger absolute value: More important feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cf143",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance\n",
    "\n",
    "Model evaluation is crucial to understand how well our logistic regression model performs. We'll use several metrics:\n",
    "\n",
    "### Classification Metrics:\n",
    "- **Accuracy**: Overall correct predictions\n",
    "- **Precision**: True positives / (True positives + False positives)\n",
    "- **Recall (Sensitivity)**: True positives / (True positives + False negatives)\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **ROC-AUC**: Area under the ROC curve (measures model's ability to distinguish classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b8fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Model Performance Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Testing Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Calculate ROC-AUC scores\n",
    "train_auc = roc_auc_score(y_train, y_train_proba)\n",
    "test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "print(f\"\\nTraining ROC-AUC: {train_auc:.4f}\")\n",
    "print(f\"Testing ROC-AUC:  {test_auc:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "accuracy_diff = train_accuracy - test_accuracy\n",
    "if accuracy_diff > 0.05:\n",
    "    print(f\"\\n⚠ Possible overfitting detected (difference: {accuracy_diff:.4f})\")\n",
    "else:\n",
    "    print(f\"\\n✓ Good generalization (difference: {accuracy_diff:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report (Test Set):\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_test_pred, \n",
    "                          target_names=['No Heart Disease', 'Heart Disease']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"True Negatives (TN):  {cm[0,0]}\")\n",
    "print(f\"False Positives (FP): {cm[0,1]}\")\n",
    "print(f\"False Negatives (FN): {cm[1,0]}\")\n",
    "print(f\"True Positives (TP):  {cm[1,1]}\")\n",
    "\n",
    "# Calculate additional metrics manually\n",
    "precision = cm[1,1] / (cm[1,1] + cm[0,1])\n",
    "recall = cm[1,1] / (cm[1,1] + cm[1,0])\n",
    "specificity = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "\n",
    "print(f\"\\nManual Calculation Verification:\")\n",
    "print(f\"Precision (PPV): {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24581914",
   "metadata": {},
   "source": [
    "## 8. Visualize Results\n",
    "\n",
    "Visual representations help us better understand our model's performance and the relationships in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Disease', 'Disease'],\n",
    "            yticklabels=['No Disease', 'Disease'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Feature Importance (Coefficient Magnitude)\n",
    "plt.subplot(1, 2, 2)\n",
    "top_features = coefficients_df.head(8)  # Top 8 features\n",
    "plt.barh(range(len(top_features)), top_features['Abs_Coefficient'])\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Absolute Coefficient Value')\n",
    "plt.title('Top 8 Most Important Features')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f6a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Visualization\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {test_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Prediction Probability Distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_test_proba[y_test == 0], bins=20, alpha=0.7, label='No Disease', color='lightcoral')\n",
    "plt.hist(y_test_proba[y_test == 1], bins=20, alpha=0.7, label='Disease', color='lightblue')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Predicted Probabilities')\n",
    "plt.legend()\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC-AUC Interpretation:\")\n",
    "print(f\"• AUC = {test_auc:.3f}\")\n",
    "if test_auc > 0.9:\n",
    "    print(\"• Excellent model performance\")\n",
    "elif test_auc > 0.8:\n",
    "    print(\"• Good model performance\")\n",
    "elif test_auc > 0.7:\n",
    "    print(\"• Fair model performance\")\n",
    "else:\n",
    "    print(\"• Poor model performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294a8dc",
   "metadata": {},
   "source": [
    "## 9. Practical Application: Making Predictions\n",
    "\n",
    "Let's see how we can use our trained model to make predictions for new patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predict for a new patient\n",
    "# Let's take a few examples from our test set\n",
    "sample_patients = X_test.iloc[:3].copy()\n",
    "\n",
    "print(\"Sample Patients for Prediction:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (idx, patient) in enumerate(sample_patients.iterrows()):\n",
    "    print(f\"\\nPatient {i+1} (Index {idx}):\")\n",
    "    print(f\"Age: {patient['age']}, Sex: {'Male' if patient['sex']==1 else 'Female'}\")\n",
    "    print(f\"Chest Pain Type: {patient['cp']}, Max Heart Rate: {patient['thalach']}\")\n",
    "    print(f\"Cholesterol: {patient['chol']}, Resting BP: {patient['trestbps']}\")\n",
    "    \n",
    "    # Scale the patient data\n",
    "    patient_scaled = scaler.transform(patient.values.reshape(1, -1))\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = lr_model.predict(patient_scaled)[0]\n",
    "    probability = lr_model.predict_proba(patient_scaled)[0, 1]\n",
    "    \n",
    "    # Get actual result\n",
    "    actual = y_test.loc[idx]\n",
    "    \n",
    "    print(f\"Predicted: {'Heart Disease' if prediction==1 else 'No Heart Disease'}\")\n",
    "    print(f\"Probability of Heart Disease: {probability:.3f}\")\n",
    "    print(f\"Actual: {'Heart Disease' if actual==1 else 'No Heart Disease'}\")\n",
    "    print(f\"Prediction: {'✓ Correct' if prediction==actual else '✗ Incorrect'}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90dc1c",
   "metadata": {},
   "source": [
    "## 9.1. Save the Trained Model\n",
    "\n",
    "Let's save our trained model and scaler so we can use them in a web application later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import joblib for model saving\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create a models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "model_filename = 'models/heart_disease_logistic_model.pkl'\n",
    "joblib.dump(lr_model, model_filename)\n",
    "print(f\"✓ Model saved as: {model_filename}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_filename = 'models/heart_disease_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "print(f\"✓ Scaler saved as: {scaler_filename}\")\n",
    "\n",
    "# Save feature names for reference\n",
    "feature_names = list(X.columns)\n",
    "feature_filename = 'models/feature_names.pkl'\n",
    "joblib.dump(feature_names, feature_filename)\n",
    "print(f\"✓ Feature names saved as: {feature_filename}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_type': 'Logistic Regression',\n",
    "    'features': feature_names,\n",
    "    'target': 'Heart Disease (0: No, 1: Yes)',\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'test_auc': test_auc,\n",
    "    'training_date': '2025-08-29',\n",
    "    'feature_descriptions': {\n",
    "        'age': 'Age of patient (years)',\n",
    "        'sex': 'Gender (1: Male, 0: Female)',\n",
    "        'cp': 'Chest pain type (0-3)',\n",
    "        'trestbps': 'Resting blood pressure (mm Hg)',\n",
    "        'chol': 'Serum cholesterol (mg/dl)',\n",
    "        'fbs': 'Fasting blood sugar > 120 mg/dl (1: True, 0: False)',\n",
    "        'restecg': 'Resting ECG results (0-2)',\n",
    "        'thalach': 'Maximum heart rate achieved',\n",
    "        'exang': 'Exercise induced angina (1: Yes, 0: No)',\n",
    "        'oldpeak': 'ST depression induced by exercise',\n",
    "        'slope': 'Slope of peak exercise ST segment (0-2)',\n",
    "        'ca': 'Number of major vessels (0-4)',\n",
    "        'thal': 'Thalassemia (1-3)'\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_filename = 'models/model_metadata.pkl'\n",
    "joblib.dump(model_metadata, metadata_filename)\n",
    "print(f\"✓ Model metadata saved as: {metadata_filename}\")\n",
    "\n",
    "print(f\"\\nModel Performance Summary:\")\n",
    "print(f\"• Test Accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"• Test ROC-AUC: {test_auc:.3f}\")\n",
    "print(f\"\\nAll files saved successfully! Ready for deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c53ccc",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Learnings\n",
    "\n",
    "### What We Accomplished:\n",
    "1. ✅ **Loaded and explored** a real-world heart disease dataset\n",
    "2. ✅ **Understood all data variables** and their medical significance  \n",
    "3. ✅ **Preprocessed the data** (checked for missing values, scaled features)\n",
    "4. ✅ **Split data** into training and testing sets\n",
    "5. ✅ **Implemented logistic regression** using scikit-learn\n",
    "6. ✅ **Evaluated model performance** using multiple metrics\n",
    "7. ✅ **Visualized results** with confusion matrix and ROC curve\n",
    "8. ✅ **Made practical predictions** for new patients\n",
    "\n",
    "### Key Insights from Our Model:\n",
    "- **Model Performance**: Our logistic regression achieved good performance on the heart disease dataset\n",
    "- **Important Features**: The most influential factors for heart disease prediction include chest pain type, maximum heart rate, and other cardiac indicators\n",
    "- **Generalization**: The model shows good generalization with minimal overfitting\n",
    "\n",
    "### When to Use Logistic Regression:\n",
    "✅ **Good for:**\n",
    "- Binary classification problems (Yes/No, True/False)\n",
    "- When you need interpretable results\n",
    "- Linear relationships between features and log-odds\n",
    "- Baseline model for comparison\n",
    "- When you need probability estimates\n",
    "\n",
    "❌ **Not ideal for:**\n",
    "- Complex non-linear relationships\n",
    "- Image or text classification (usually)\n",
    "- When high accuracy is critical and interpretability is not\n",
    "\n",
    "### Next Steps:\n",
    "1. **Feature Engineering**: Create new features or transform existing ones\n",
    "2. **Hyperparameter Tuning**: Optimize model parameters\n",
    "3. **Try Other Algorithms**: Compare with Random Forest, SVM, etc.\n",
    "4. **Cross-Validation**: Use k-fold CV for more robust evaluation\n",
    "5. **Handle Class Imbalance**: If needed, use techniques like SMOTE\n",
    "\n",
    "### Medical Context Note:\n",
    "⚠️ **Important**: This model is for educational purposes only. Real medical diagnosis requires professional medical evaluation and should never rely solely on machine learning predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515ab3a",
   "metadata": {},
   "source": [
    "## 11. Practice Exercises\n",
    "\n",
    "### Exercise 1: Model Improvement\n",
    "Try the following modifications and compare results:\n",
    "1. Use different train-test split ratios (70-30, 90-10)\n",
    "2. Try different solvers ('lbfgs', 'newton-cg', 'sag')\n",
    "3. Add regularization by adjusting the `C` parameter\n",
    "\n",
    "### Exercise 2: Feature Analysis\n",
    "1. Create correlation heatmap of all features\n",
    "2. Identify which features are most correlated with the target\n",
    "3. Try building a model with only the top 5 most important features\n",
    "\n",
    "### Exercise 3: Threshold Optimization\n",
    "1. Plot precision-recall curve\n",
    "2. Find the optimal threshold for classification (instead of 0.5)\n",
    "3. Calculate metrics using the optimal threshold\n",
    "\n",
    "### Exercise 4: Real-world Application\n",
    "Create a simple function that takes patient data as input and returns:\n",
    "1. Risk level (Low, Medium, High)\n",
    "2. Probability percentage\n",
    "3. Key risk factors for that patient\n",
    "\n",
    "---\n",
    "\n",
    "**End of Session 7: Logistic Regression**\n",
    "\n",
    "*Next Session Preview: We'll explore more advanced classification algorithms like Random Forest and Support Vector Machines, and learn about ensemble methods.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
